[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESIP Biological Data Cluster (BDS) Primer Guide",
    "section": "",
    "text": "1 Introduction\nThis is still a work in progress and only presented here for the purposes of receiving feedback. This message will be removed when the Primer Guide has been officially published.\nTo paraphrase the MARCO-BOLO data management plan, the main challenge with biological data is that it comes in many flavors. For example, it may include evidence of past or present organisms based on physical samples, chemicals, organic molecules, sound, or images. By using standards to organize and share data efficiently, biologists can turn this diversity of data types into a strength rather than a challenge when dealing with biological data.\n\nThis Primer Guide\nThe first product from this cluster, the Biological Data Standards Primer, is an easy to digest resource for new biological data managers. However, it is essentially a list of names and links. We created this Primer Guide with the intention of providing additional context, bridging the gap between technical standards documentation and the lists of links available in the Primer.\n\n\n\n\nAudience\nThe target audience for this Primer Guide is anyone interested in working with biological data, from data collection, to data sharing, and data management. We hope this Primer Guide will provide useful context for using biological standards.\n\n\n\n\n\n\nKey Values\n\n\n\nIn designing this resource, we focused on a few key values.\n Helping People Climb: We want to help people learn about data standards resources. We were students too; we know how rarely this is part of a standard science education.\n Always Feeding Back: Information and standards develop too quickly to publish guidance in a journal. You can immediately let us know what isn‚Äôt working, or what is wrong, via GitHub issues. We‚Äôre excited to work with you to make this better!\n Transparent: Sometimes resources are confusing until you understand the journey of how it was created. Having experienced this ourselves, we want to share the conversations that helped us edit this resource.\n\n\n\n\nBackground on the Cluster\nThe ESIP Biological Data Standards Cluster formed in 2020 to maximize data relevance and utility for understanding changes in biodiversity over time. The cluster‚Äôs focus has expanded to include more aspects of biology beyond biodiversity, acknowledging that biological data may be useful for many facets of earth science. To accomplish this, the cluster encourages awareness and shared understanding of biological data standards by facilitating community building and information sharing via guidance, documentation, and training for the US biological data community.\n\n\nHow to contribute to this Primer Guide\nIf you would like to suggest changes or additions to the current version of the best practice documents, please use the GitHub issues to document your request.\n\nTranslations\nCurrently we only have the capacity to offer it in English. However, users can get a rough translation using Google Translate. Just drop the url in this service: https://translate.google.com/?sl=en&tl=es&op=websites",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html",
    "href": "CONTEXT_UNDERSTANDABILITY.html",
    "title": "2¬† Provide Context and Understandability to Your Data",
    "section": "",
    "text": "2.1 Ecological Metadata Language (EML) üçÉ\nMetadata standards ensure that data are described using a consistent structure and format to provide necessary context for users across the data lifecycle from data management to accessibility and interoperability. In biological data, there are a few metadata standards to be aware of: the Ecological Metadata Language (EML), ISO-19115, and MIxS.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html#ecological-metadata-language-eml",
    "href": "CONTEXT_UNDERSTANDABILITY.html#ecological-metadata-language-eml",
    "title": "2¬† Provide Context and Understandability to Your Data",
    "section": "",
    "text": "What Is It?\nEcological Metadata Language (EML) is a metadata schema (i.e.¬†standard) that was developed by the ecological community for ecological data, including biological data. Shared data can include an EML file that provides context for all files in the data ‚Äúpackage‚Äù. EML is presented in Extensible Markup Language (XML), which provides standard details for ecological data in a structure that is readable to both people and machines.\nFor more information, see the full EML documentation or the National Center for Ecological Analysis and Synthesis (NCEAS) EML GitHub repository.\n\n\nWhy?\n\nIt provides context and improves reproducibility of the data.\nIt captures important links and relationships between data, such as a time series, hierarchical taxonomies, IDs, and authoritative reference points and vocabularies.\nEML helps represent ecological information in a standardized way.\nEML is mandatory for LTER, iLTER, OBIS, GBIF, Darwin Core Archive (DwC-A) data sharing.\n\n\n\nTop Resources\nTools or packages to help write EML:\n\nFor data managers, coders:¬†\n\nEML-R package\nPostgresql database with fields compatible with EML\nR-code for generating EML from LTER-metabase (built on EML-R package)\nEMLAssemblyline (built on EML-R package)\n\nFor those not inclined to write scripts:\n\nezEML",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html#iso-19115",
    "href": "CONTEXT_UNDERSTANDABILITY.html#iso-19115",
    "title": "2¬† Provide Context and Understandability to Your Data",
    "section": "2.2 ISO 19115 üó∫Ô∏è",
    "text": "2.2 ISO 19115 üó∫Ô∏è\n\nWhat Is It?\nISO-19115 is a metadata standard, developed and maintained by the International Standards Organization (ISO), for describing geographic data. Biological data are inherently geographic, especially as we strive to understand how occurrences are impacted by ecological or environmental variables. ISO-19115 provides information about the identification, extent, quality, spatial and temporal schema, spatial reference, and distribution of geographic data. It evolved from the need for flexibility in harmonizing the Federal Geographic Data Committee (FGDC) Content Standard for Digital Geospatial Metadata (CSDGM) with other geospatial standards. For more information about implementations and extensions of ISO 19115, including for remotely sensed imagery and gridded data, see the Digital Curation Centre ISO 19115 guide or the NCEI Metadata Workbook.\n\n\nWhy?\n\nIt helps to provide important geographic context to data in a standardized way.\nUsing ISO metadata is mandatory for some U.S. federal agencies, like NOAA, NASA and USGS, to share their data through government repositories.\nIt can be used to describe individual files, data packages, and collections of datasets.\n\n\n\nTop Resources\n\nHow to Convert ISO to EML¬†\nWork Flow Model\nmdToolkit - mdEditor is a writer for ISO 19115 metadata which uses mdJSON as an intermediary and mdTranslator allows translation to different metadata formats",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html#minimum-information-about-any-x-sequence-mixs",
    "href": "CONTEXT_UNDERSTANDABILITY.html#minimum-information-about-any-x-sequence-mixs",
    "title": "2¬† Provide Context and Understandability to Your Data",
    "section": "2.3 Minimum Information about any (x) Sequence (MIxS) üß¨",
    "text": "2.3 Minimum Information about any (x) Sequence (MIxS) üß¨\n\nWhat Is It?\nMIxS (pronounced MIX-ess) is a set of checklists and packages for molecular genomic sequence data, such as DNA and RNA. MIxS is a standard published by the Genome Standards Consortium (GSC) for molecular biologists and ecologists who create, manage, and archive sequence data. It includes a breadth of environment-specific metadata variables (e.g.¬†soil variables) to augment genome-specific checklists (e.g.¬†bacteria) and enables interoperability with environmental analyses.\n\n\nWhy?\n\nIt helps to provide standardized metadata about genetic sequence data.\nIt is used by the International Nucleotide Sequence Database Collection (INSDC), which has the following member participating databases: ROIS - NIG, EMBL-EBI and NCBI.\n\n\n\nTop Resources\n\nMIxS Term Search Tool\nGenomic Standards Consortium term list\nMinimum Information about Marker Gene Sequence (MIMARKS)\nMIxS is maintained by the community using GitHub. To propose changes, ask questions, see the MIxS GitHub repository.\nMinimum Information about Sequence Data from the Built Environment (MIxS-BE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "INTEGRATE_DATA.html",
    "href": "INTEGRATE_DATA.html",
    "title": "3¬† Integrate Your Data with Other Data",
    "section": "",
    "text": "3.1 Darwin Core üêò\nBiodiversity data are collected and managed in many different systems and environments from museum collections, to environmental monitoring programs, research programs, or community science projects (e.g.¬†iNaturalist). Data are often heterogeneous within and across these systems, depending on research objectives, but there are consistent themes in them: the what, where, and when. Although the details may have names unique to your own data, there are specific field names to use to provide these details in a way that aligns with others‚Äô data. Through the use of common terminology, downstream users can not only better search for and discover data, but also evaluate, integrate, and compare datasets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Integrate Your Data with Other Data</span>"
    ]
  },
  {
    "objectID": "INTEGRATE_DATA.html#darwin-core",
    "href": "INTEGRATE_DATA.html#darwin-core",
    "title": "3¬† Integrate Your Data with Other Data",
    "section": "",
    "text": "What Is It?¬†\nDarwin Core (DwC) is a data standard that offers a stable, simple, and flexible framework for compilation and reuse of biodiversity data, including observations, specimens, samples, and related information, from varied and variable sources. DwC builds upon Dublin Core, a set of metadata terms used by libraries to describe physical and digital resources, to describe biological occurrences. The DwC glossary of terms provides identifiers, labels, and definitions to map occurrence information from multiple sources in a cohesive and interpretable way.¬†\nA single dataset, known as the Darwin Core Archive (DwC-A), is a compressed (e.g.¬†zip) file that contains interconnected text files (e.g.¬†csv or tsv) with DarwinCore standard-mapped data that are arranged into core files. To facilitate human- and machine-interpretation of the data, xml files are included to describe the contents of the Archive, the relationships between the core files and connected tables contained in the DwC-A.\n\n\nWhy?\n\nAny observation of an or organism can be standardized to Darwin Core irrespective of the observing method by which the data were collected (e.g., observational data, genomics, imaging, animal tracking).¬†\nThe Darwin Core standard plays a fundamental role in facilitating open-access biodiversity data sharing, use, and reuse.¬†The Global Biodiversity Information Facility (GBIF), Ocean Biodiversity Information System (OBIS), the Atlas of Living Australia (ALA), and many more repositories use this data standard.\nAligning your data to DwC facilitates the use of heterogeneous biological data gathered using disparate collection methods.\n\n\n\nTop Resources\n\nDarwin Core Quick Reference Guide.\nWieczorek et al., (2012) Darwin Core: An Evolving Community-Developed Biodiversity Data Standard.\nBiddle, M., Benson, A., van der Stap, T., Pye, J., Murray, T., Lawrence, E., & Formel, S. (2024). Marine data mobilization workshop (Version 2024). Zenodo.¬†https://doi.org/10.5281/zenodo.11085142\nFor more information, see overviews of CF as a presentation and paper. You can also check out the presentation John Weiczorek gave to our ESIP cluster back in April, 2022:",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Integrate Your Data with Other Data</span>"
    ]
  },
  {
    "objectID": "INTEGRATE_DATA.html#climate-and-forecast-cf",
    "href": "INTEGRATE_DATA.html#climate-and-forecast-cf",
    "title": "3¬† Integrate Your Data with Other Data",
    "section": "3.2 Climate and Forecast (CF) üåç",
    "text": "3.2 Climate and Forecast (CF) üåç\n\nWhat Is It?\nThe Climate and Forecast (CF) Metadata Conventions provide a framework to clarify details about the context of each piece of data: a description of what the data in each variable represents and their spatial and temporal properties. CF was designed to represent geolocations and times of Earth science data, the physical quantities that the data represent, and other ancillary information.\nAlthough CF is well-established, the original intended purpose that has guided it‚Äôs development means it can conflict with traditional, or intuitive ideas about describing biological data. Still, terms from CF are often applicable to peripheral context of biological data.\nLike many standards, CF is built on other standards. For example, units are specified using the UDUNITS system and files formats are defined by the netCDF (Network Common Data Form) standard. These standards have been around for about 30 years, and have facilitated the efficient processing and sharing of climate and oceanographic data.\n\n\nWhy?\n\nIf you‚Äôre a biologist who works in the realm of oceanography and climate science, becoming familiar with CF conventions is a good idea because it will help you hit the ground running with data from the National Centers for Environmental Information (NCEI) and NASA EarthData repositories (For a longer list or repositories, see here)\nIt‚Äôs also worthwhile to become familiar with CF because it is a reasonably long-lived and tested standard. It has lasted through the ups and downs of technological and scientific progress, which is a tribute to the utility of the standard and the community behind it.\n\n\n\nTop Resources\n\nThe NERC Vocabulary Server hosts CF and maps it to other vocabularies.¬†\nTo find standard names that describe your data, open up the latest Standard Name table (as HTML or XML) and search through it for words typically used for your data.\nCF is developed and maintained by the community using GitHub. To propose changes, ask questions, see the CF GitHub repositories.\nFor more information, see overviews of CF as a presentation and paper. You can also check out the presentation Roy Lowry gave to our ESIP cluster back in April, 2022:",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Integrate Your Data with Other Data</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html",
    "href": "INTEROPERABLE_DATA.html",
    "title": "4¬† Make Your Data Interoperable",
    "section": "",
    "text": "4.1 Controlled Vocabularies\n[whole section preamble about ‚ÄúSEMANTIC RESOURCES‚Äù (rather than controlled vocabularies, as it was)‚Ä¶ Possibly want to connect to framework of understanding, incl.¬†controlled vocabulary, taxonomy, close associations, and ontology = taxonomy + relationships between terms]\nPossible content/phrases to incorporate into whole section preamble: Ontology describes a set of concepts or categories in a domain and shows their properties and the relations between them.\nSemantic resources (i.e.¬†______), improve consistency across diverse sources of data and are essential to interoperability. Controlled vocabularies, taxonomic authorities, and habitat classifications are all useful semantic resources for standardizing biological data.\nControlled vocabularies provide standardized terms and definitions for biological concepts\nThey facilitate searching for data in web portals. They also enable records to be interpreted by computers. This opens up data sets to a whole world of possibilities for automated data workflows, computer aided manipulation, distribution, interoperability, and long-term reuse.\nBy connecting terms held in controlled lists using standards, the data described by these controlled vocabularies become more interoperable and hence more broadly reusable.\nenable records to be interpreted by computers, facilitating automated data workflows, computer aided manipulation, distribution, and interoperability\nIt becomes possible to build a truly distributed and interoperable data ecosystem across domain boundaries, enabling data reuse no matter the purpose for which they were collected in the first place.\nconcise, controlled description\nStandardized classifications of observational data also allows the information derived from individual projects to be compared with each other and compiled into larger and more extensive representations across space and time. This advances the collective scientific knowledge of natural systems and enables coordinated management activities, such as regional assessments, monitoring and restoration between partners.\nCombined with international ontologies like ENVO, which is used by UNESCO, for example, the reach of your data will become global.\nControlled vocabularies are pre-determined standardized terms and definitions used to describe a specific entity, collection, parameter, or unit of measurement in either metadata or data. While they facilitate computer-readability, they also reduce ambiguity around terms. Controlled vocabularies are designed to fit specific schema and are routinely updated by the communities that use them.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#controlled-vocabularies",
    "href": "INTEROPERABLE_DATA.html#controlled-vocabularies",
    "title": "4¬† Make Your Data Interoperable",
    "section": "",
    "text": "4.1.1 Environmental Ontology (ENVO) [emoji]\n\n\nWhat is it?\nThe Environmental Ontology (ENVO) is an ontology (i.e.¬†it describes a set of concepts or categories, their properties, and the relations between them) of all things environmental (e.g.¬†systems, components, and processes). ENVO aids humans, machines, and semantic web applications in understanding environmental entities of all kinds, from microscopic to intergalactic scales, increasing the interoperability of environmental descriptions.\n\n\nWhy?\n\nENVO provides a comprehensive standardized vocabulary for describing habitats, ecosystems, and environmental processes.\nIt incorporates the relationships between objects (Ontology).\nIt is used internationally.\nSeveral portals and ontology browsing interfaces already harvest from ENVO.\n\n\n\nTop Resources\n\nButtigieg et al.¬†(2013) article introducing ENVO in Journal of Biomedical Semantics\nButtigieg et al.¬†(2016) article revisiting ENVO\nInstructions on how to Browse ENVO terms\n\n[Do we want to include this fun fact?\nENVO terms can also be used within the MIxS metadata standard (see here), for example to describe the materials that compose your sample or the environment where the sample was collected.\nCould be included in either and link to the other]\n\n\n4.1.2 Natural Environmental Research Council (NERC) Vocabulary Server (NVS) [emoji]\n\n\nWhat is it?\nThe Natural Environment Research Council (NERC)-funded Vocabulary Server (NVS) provides access to standardized and hierarchically-organized vocabularies, primarily in oceanographic and associated domains. NVS is managed by the British Oceanographic Data Centre at the National Oceanography Centre (NOC).\n\n\nWhy?\n\nIt is used by the marine science community in the UK (MEDIN), Europe (SeaDataNet), and globally, by a variety of organisations and networks.\nBy connecting terms held in controlled lists using standards, the data described by these controlled vocabularies become more interoperable and hence more broadly reusable.\nIt becomes possible to build a truly distributed and interoperable data ecosystem across domain boundaries, enabling data reuse no matter the purpose for which they were collected in the first place.\n\n\n\nTop Resources\n\nThe [NERC Vocabulary Server] (https://vocab.nerc.ac.uk/)\nThe NVS Search can be used to find controlled vocabulary terms\nThe NVS Vocab Search can search the entire NVS content\nSeaDataNet Search searches only collections used by the SeaDataNet data infrastructure\n\n\n\n4.1.3 Global Change Master Directory (GCMD) Keywords [emoji.. gonna suggest a globe]\n\n\nWhat is it?\nThe Global Change Master Directory (GCMD) Keywords are a standardized set of terms used to describe Earth science data sets and services. They serve as a common language for categorizing and searching for data related to Earth science, environmental science, and global change research\n\n\nWhy?\n\nGCMD Keywords provide a standardized vocabulary for describing Earth science data, ensuring consistency and interoperability across different data sets and repositories.\nThe GCMD keywords describe Earth science data and services consistently and comprehensively in a hierarchical format and follow a codified governance process.\nThe power of the keywords is in their ability to enable scientists to tag their data using a taxonomy of controlled scientific categories. This, in turn, allows those searching for data to discover datasets easily through the use of an established hierarchy.\n\n\n\nTop Resources\n\nMore information can be found here:Global Change Master Directory (GCMD) Keywords | Earthdata (nasa.gov)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#taxonomic-authorities",
    "href": "INTEROPERABLE_DATA.html#taxonomic-authorities",
    "title": "4¬† Make Your Data Interoperable",
    "section": "4.2 Taxonomic Authorities",
    "text": "4.2 Taxonomic Authorities\n[preamble for Taxonomic Authorities]\n\n4.2.1 Catalogue of Life (COL) [emoji]\n\n\nWhat Is It?\nCatalogue of Life describes itself as, ‚ÄúThe most complete authoritative list of the world‚Äôs species - maintained by hundreds of global taxonomists.‚Äù It brings together information from taxonomists, and taxonomic databases, to construct an integrated view of currently accepted species across all taxonomic groups. A list of source datasets can be found here.\n\nThe primary mission of COL is to deliver data, but the tools and services offered by COL also enable taxonomists and other stakeholders to publish and revise species lists for any purpose.\n\n\nWhy?\n\nCOL adds persistent identifiers that enable users to track changes to a scientific name.¬†\nCOL helps downstream users consider the most up-to-date past and current characteristics of an organism: its biology, distribution, relevance to humans, and evolutionary history.\n\n\n\nTop Resources\n\nUsers can browse the COL Checklist, which is updated monthly. COL pulls information from specific data sources, e.g.¬†FishBase (see: https://www.catalogueoflife.org/data/taxon/49JFH).¬†\nCOL data pipeline for COL taxonomic checklist data: https://www.catalogueoflife.org/about/colpipeline.¬†\nCOL ChecklistBank API: https://api.checklistbank.org/.\n\n\n\n4.2.2 Integrated Taxonomic Information System (ITIS) [emoji]\n\n\nWhat Is It?\nITIS is a taxonomic database, developed and maintained by a partnership of federal agencies that provides reliable information on the nomenclature, taxonomy, and distribution of 1.8 million species of plants, animals, fungi and microbes in North America and the world. ITIS couples each scientific name with a unique taxonomic serial number (TSN), which ensures consistency and accuracy in the naming and classification of species.\n\n\nWhy?\n\nUnique taxonomic serial number (TSN) ensures consistent and accurate naming and classification of species\nITIS is an important tool for identifying and cataloging species and monitoring their populations.¬†\n\n\n\nTop Resources\n\nITIS website¬†\nITIS API\n\n\n\n4.2.3 Paleobiology Database (PBDB) [emoji]\n\n\nWhat Is It?\nThe Paleobiology Database (PBDB) is an online, expert-curated database that aims to provide taxonomic information for paleobiological taxa of all geological ages. It contains data for almost half a million paleobiological taxa from over 900 different contributors.\n\n\nWhy?\n\nChecking your paleobiological taxonomic names against the PBDB will ensure the names are up-to-date based on current taxonomic literature as reflected in the database.\nPBDB provides the taxonomic backbone to the Global Biodiversity Information Facility (GBIF). This means aligning your taxonomic names with PBDB will make the process of sharing your data easier.\n\n\n\nTop Resources\n\nPBDB can be accessed via their website, mobile applications, and an API.\nThe PBDB website has a Resources tab where more information about these access points can be found.¬†\nThe same Resources page also includes information on how to contribute taxonomic information to PBDB. Lessons are available via a playlist of YouTube videos. Note that you will need to be granted Contributory User access to contribute to PBDB. More information about the process and different user types is available in the PBDB User Guide.\n\n\n\n4.2.4 World Register of Marine Species (WoRMS) üêã\n\n\nWhat Is It?\nThe World Register of Marine Species (WoRMS) is an authoritative and comprehensive list of names of marine organisms. It provides detailed information about marine species from around the world, helping anyone interested in marine life to find accurate and up-to-date information about these species, including where they can be found, their characteristics, and how they are related to one another.¬†\n\n\nWhy?\n\nWoRMS content is curated by taxonomic and thematic marine experts.¬†\nEach taxonomic group is represented by an expert who has the authority over the content, and is responsible for controlling the quality of the information. Each of these main taxonomic editors can invite several specialists of smaller groups within their area of responsibility to join them.¬†\nWoRMS is the taxonomic database used by OBIS, and other important biological initiatives.\n\n\n\nTop Resources\n\nWoRMS and its associated tools can be explored via web browser or the WoRMS API\nR packages: worrms, taxize\nPython package",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#habitat-classification",
    "href": "INTEROPERABLE_DATA.html#habitat-classification",
    "title": "4¬† Make Your Data Interoperable",
    "section": "4.3 Habitat Classification",
    "text": "4.3 Habitat Classification\nHabitat classification is the process of organizing quantitative observations (i.e., data collected by various methods and instruments) about the natural world into meaningful, human-understandable representations and descriptions. Habitat classification standards or systems provide terminology and methodology or guidance so that classification can be performed in a standard manner by individual projects and programs.\n\n4.3.1 Coastal and Marine Ecological Classification Standard (CMECS) [emoji]\n\n\nWhat is it?\nThe Coastal and Marine Ecological Classification Standard (CMECS) is a structured dictionary of terms that describe benthic habitats in marine, estuarine, and lacustrine settings. It was developed by a consortium of scientists and coastal managers to meet the needs of inventorying, monitoring and managing natural resources in US and territorial waters. CMECS terms are organized in a spatially-scaled framework for annotating geospatial data and integrating information about the physical components of aquatic habitats that enables three-dimensional representations of ecological conditions for associating with faunal behavior.\n\n\nWhy?\n\nCMECS is endorsed as a Federal Geographic Data Committee (FGDC) standard in the U.S. for aquatic environmental data so that data collectors and analysts can describe data using standard terminology and organization structures.\nUsing CMECS enables data discovery, data use and re-use, and broader analytical applications of data federally-funded data assets.\n\n\n\nTop Resources\n\n[Documentation] (https://iocm.noaa.gov/standards/cmecs-home) of the CMECS standard, how to use it (including examples), and how to contribute to its improvement\n\n\n\n4.3.2 U.S. National Vegetation Classification (NVCS) [emoji]\n\n\nWhat is it?\nThe U.S National Vegetation Classification (USNVC) is the comprehensive, standardized, and hierarchical classification system for all vegetation types in the United States. Because several agencies, each with its own sampling protocols, are tasked with mapping and describing vegetation in the United States, the resultant inventories are not automatically interoperable, making vegetation resource monitoring across jurisdictional boundaries challenging. The USNVC, a collaboration between the Ecological Society of America (ESA), NatureServe, and various federal agencies, was created to address this need. It provides a common language that allows for communication and cooperation on vegetation management issues across jurisdictional boundaries for the effective management and conservation of plant communities.\nVegetation modeling and mapping are relevant to many conservation efforts, including land inventories, wildlife habitat inventories, enhancing natural resource conservation efforts, fire management, invasive species management, and setting national vegetation policies (e.g.¬†biofuels, carbon markets, and ecosystem services).\n\n\nWhy?\n\nAs a dynamic standard, the USNVC is designed to be easily adapted as new ecological knowledge becomes available.\nIts hierarchical nature makes classification scalable for diverse applications from vegetation monitoring to broad-scale analyses of trends across North America.\nThe USNVC is governed by standards for vegetation data collection and analysis, ensuring consistent reporting on the nation‚Äôs vegetation resources.\n\n\n\nTop Resources\n\nOverview of the USNVC Database\nESA‚Äôs collection of USNVC resources, including fact sheets, presentations, webinars, and posters.\n\n\n\n4.3.3 National Wetlands Classification System (?) (NWCS) [emoji]\n\n\nWhat is it?\nThe primary objective of the Classification of Wetlands and Deepwater Habitats of the United States, as originally drafted by Cowardin et al.¬†(1979:3), was ‚Äúto impose boundaries on natural ecosystems for the purposes of inventory, evaluation, and management.‚Äù The FGDC Wetlands Classification Standard (WCS) provides minimum requirements and guidelines for classification of both wetlands and deepwater habitats that are consistent with the FGDC Wetlands Mapping Standard (FGDC-STD-015-2009).¬†\n\n\nWhy?\n\nNWCS was developed to support a detailed inventory and periodic monitoring of the Nation‚Äôs wet habitats using remote sensing.\nIt has been an official National Standard since 1996 (FGDC-STD-004), and has been the de facto standard for mapping U.S. wetlands and deepwater habitats since 1976\nThe NVC and Wetlands standard is endorsed as a Federal Geographic Data Committee (FGDC) standard in the U.S. for aquatic environmental data so that data collectors and analysts can describe data using standard terminology and organization structures.\n\n\n\nTop Resources\n\nWetland Classification codes in table and tool formats\nThe second edition of Classification of Wetlands and Deepwater Habitats of the United States, which outlines the underlying concepts, definitions, systems, and sub-systems.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html",
    "title": "5¬† Make Your Data Internet Ready",
    "section": "",
    "text": "5.1 Web-enabled standards\nWeb services and standards are useful to understand if you collect or manage biological data. Platforms like NOAA‚Äôs NCEI, NASA, OBIS, and GBIF, etc. utilize standard web services to serve data. Web-friendly data standards facilitate the transfer and handling of data via web services by making information visible in a predictable way, promoting online sharing, programmatic discovery, access, and processing of data across platforms and disciplines.\nWeb standards are the formal, non-proprietary standards and other technical specifications that define and describe different aspects of the World Wide Web. Web standards are created by standards bodies, which are institutions that invite groups of people to come together and agree on how the technologies should work in the best way to fulfill specific use cases. Web standards are key to global data discovery.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#web-enabled-standards",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#web-enabled-standards",
    "title": "5¬† Make Your Data Internet Ready",
    "section": "",
    "text": "5.1.1 W3C standards\n\n\nWhat Is It?\nThe World Wide Web Consortium (W3C) develops the W3C standards, which serve as building blocks to build internet browsers, web pages, blogs, search engines, and other software that power our experience on the web. Although HTML is its cornerstone, W3C publishes a range of technical reports, which help move the web forward, like CSS, SVG, WOFF, WebRTC, XML, and a growing variety of APIs.\n\n\nWhy?\n\nDevelopers can create interactive experiences available on any device.\nData can be made more FAIR by increasing your awareness of these standards.\n\n\n\nTop Resources\n\nW3Schools offers a variety of tutorials for free",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#section",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#section",
    "title": "5¬† Make Your Data Internet Ready",
    "section": "5.2 ",
    "text": "5.2 \n\n5.1.2 Dublin Core Standard\n\n\nWhat Is It?\nDublin Core is a metadata standard of 15 ‚Äòcore‚Äô terms originally developed for archives and libraries to describe physical or digital resources and details about their collection. Darwin Core is an extension of Dublin Core for biodiversity information.\n\n\nWhy?\n\nCrowd source: Why is it beneficial to know about Dublin Core for internet ready data?\n\n\n\nTop Resources\n\nDisambiguating the Cores presentation\nDublin Core Metadata Initiative documentation\n\n\n\n5.1.3 DataCite\n\n\nWhat Is It?\nThe DataCite metadata schema is an international, not-for-profit organization which aims to improve data citation through web-enabled standards that connect products and citations.¬†\n\n\nWhy?\n\nHelps mint persistent identifiers, such as digital object identifiers (DOI), for research products, which enables data archiving and long-term preservation\nHelps connect the research product to researchers through other persistent identifiers, such as ORCIDs for researchers or ROR for organizations\nPromotes long-term preservation, accessibility, reuse, and attribution of research products with citable contributions to a scholarly record\nConnects users and publishing machinery\n\n\n\nTop Resources\n\nREST API, which enables retrieval, creation, and update of a DOI metadata record.\nAdditional documentation on DataCite\n\n\n\n5.1.4 Schema.org\n\n\nWhat Is It?\nSchema.org provides documentation on a set of extensible schemas, which are schemas where users can use components to create other schemas. This enables users to embed structured data on their web pages to help search engines understand the information presented and provide richer search results. Using schema.org vocabulary as well as various formats (e.g., JSON-LD) to mark up website content with metadata about itself, makes it easier for websites or data records to not only be searched but also for the relationships between them to be understood.\n\n\nWhy?\n\nMake your research more easily and prominently discvoerable through major search engines.\n\n\n\nTop Resources\n\nYou can add schema.org markup to your webpages or records using various online tools, including Google‚Äôs Structured Data Markup Helper, or by directly adding code to your webpages.¬†\nDocumentation on schema.org",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#web-services",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#web-services",
    "title": "5¬† Make Your Data Internet Ready",
    "section": "5.3 Web Services",
    "text": "5.3 Web Services\nWeb services run much of our digital world today. You probably use them through your phone every day, without noticing a thing. You can think of a web service as a waiter at a restaurant. You (the user) order food (a request), the waiter (the web service) takes your order to the kitchen (the server or application), and then brings you back your food (the response). This allows different parts of a computer system or different systems altogether to interact without needing to know how each other works internally. When web services are fully utilized, it results in impressive hi-speed analysis, like the analytics shared during football (all types üôÇ) games, the olympics, and other sporting events.\n\n5.2.1 ERDDAP‚Ñ¢ Web Service\n\n\nWhat Is It?\nERDDAP‚Ñ¢ (pronounced ur-dap) is a data server that offers users a simple and consistent way to download, integrate, analyze, visualize, and map multiple scientific datasets from different sources and scientific communities ‚Äì typically oceanographic and atmospheric data.\nTo facilitate comparisons of data from different datasets, requests and results in ERDDAP‚Ñ¢ use standardized space/time axis, which makes it easier for users to specify data constraints in requests without having to worry about the data format. ERDDAP‚Ñ¢ allows users to request a subset of a dataset, and can convert the subset to a desired file format such as .csv, .json, .nc and others, for download.\n\n\nWhy?\n\nERDDAP‚Ñ¢ is free, open source, and used globally\nAll information, data, and figures made available via ERDDAP‚Ñ¢ are also available via an API, making data programmatically accessible.\nERDDAP‚Ñ¢ has a RESTful web service which is designed to be easy for computer programs and scripts to use or interact with.\nUsed for oceanographic and atmospheric datasets, but also works great for biological and biodiversity-relevant observations\nGood for both gridded and tabular data - See table dataset API docs here, and for gridded datasets here.\n\n\n\nTop Resources\n\nCoastWatch Training and specifically ERDDAP basics\nAwesome ERDDAP\nOverview: Distributed Model Data Access\nData providers can set up their own ERDDAP server to serve up their data.\nAdditional overall documentation on ERDDAP can be found here.¬†¬†\n\n\n\n5.2.2 Thematic Real-time Environmental Distributed Data Services (THREDDS)\n\n\nWhat Is It?\nThe THREDDS server, which was developed prior to ERDDAP, has features and interfaces that makes it easier to explore and use data.¬†\n\n\nWhy?\n\nCrowd source: What is beneficial about THREDDS for internet ready data?\n\n\n\nTop Resources\n\nA comparison of ERDDAP and THREDDS\n\n\n\n5.2.3 Web Map Service\n\n\nWhat Is It?\nA Web Map Service (WMS) is a way to retrieve georegistered map images over the internet to display in applications and web pages. The WMS specifications were developed by the Open Geospatial Consortium (OGC) to enable interoperability and use in web browsers, open-source GIS software (ex. QGIS), and proprietary GIS software (ex. Esri).\n\n\nWhy?\n\nWMS allows you to view and use maps from different sources that host the maps and data used to create them without needing to download them.\n\n\n\nTop Resources\n\nCrowd source: What is beneficial about Web Map Services for internet ready data?",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html",
    "href": "SOFTWARE_READY.html",
    "title": "6¬† Make Your Data Software Ready",
    "section": "",
    "text": "6.1 Use non-proprietary formats\nThose sharing or managing data can take small steps to make them ‚Äúsoftware ready.‚Äù These include using non-proprietary formats, structuring tables with specific columns and entries, including standards for information about time, place, and organism.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#use-non-proprietary-formats",
    "href": "SOFTWARE_READY.html#use-non-proprietary-formats",
    "title": "6¬† Make Your Data Software Ready",
    "section": "",
    "text": "What is it?\nNon-proprietary file formats do not require specific software and can be accessed without licenses and within different software systems. For example, comma separated values (CSV) format is becoming an increasingly popular non-proprietary format compared to the proprietary .xlsx format.\n\n\nWhy?\n\nAllows data to be useful in perpetuity by ensuring data readability and reusability across multiple platforms\nAligns better with the FAIR data principles\nSupports open science.\nMany applications (e.g.¬†Microsoft Office) allow exporting into multiple formats, which makes it easy to share data in non-proprietary formats even if it was created using proprietary software.\n\n\n\nTop Resources\n\nTable of commonly used formats for common data types\nA more detailed table that is specific to U.S. Federal records management",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#structure-tabular-data-in-tidylong-format",
    "href": "SOFTWARE_READY.html#structure-tabular-data-in-tidylong-format",
    "title": "6¬† Make Your Data Software Ready",
    "section": "6.2 Structure tabular data in tidy/long format",
    "text": "6.2 Structure tabular data in tidy/long format\n\nWhat is it?\nLong (or sometimes called ‚Äútidy‚Äù) format for tabular data can best be described as having one observation per row.\nThe following example shows two different formats ‚Äì wide and long ‚Äì of the same data. Notice that while sites 1, 2, and 3 are the column names filled with counts for each species in the wide format, site and count become the column names in long format.\n\n\nWhy?\n\nThe clear structure makes data more machine readable, particularly with commonly-used analytical software.\nData are as atomic as possible (e.g.¬†no mixed types in one field)\nIt is easier to aggregate data across multiple files\n\n\nExample of Wide Format\n\n\nspecies\nsite_01\nsite_02\nsite_03\n\n\n\n\nTilia americana\n4\n2\n4\n\n\nPinus strobus\n3\n3\n3\n\n\n\n\nExample of Long Format\n\n\nspecies\nsite\ncount\n\n\n\n\nTilia americana\nsite_01\n4\n\n\nTilia americana\nsite_02\n2\n\n\nTilia americana\nsite_03\n4\n\n\nPinus strobus\nsite_01\n3\n\n\nPinus strobus\nsite_02\n3\n\n\nPinus strobus\nsite_03\n3\n\n\n\n\n\nTop Resources\n\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1‚Äì23.\n\nVideo: Data Sharing and Management Snafu in 3 Short Acts (video)\nTips for working with data in BASH\nData Organization in Spreadsheets for Ecologists\nCleaning Data and Quality Control",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#follow-iso-8601-for-dates",
    "href": "SOFTWARE_READY.html#follow-iso-8601-for-dates",
    "title": "6¬† Make Your Data Software Ready",
    "section": "6.3 Follow ISO 8601 for dates",
    "text": "6.3 Follow ISO 8601 for dates\n\nWhat is it?\nISO 8601 is a convention for dates and times, where dates are listed as YYYY-MM-DD and time is given in Coordinated Universal Time (UTC, Zulu, or GMT) which is the time standard, relative to 0 longitude, that regulates global clocks.\nThe following table outlines how to write dates, times, and time intervals using ISO 8601:\n\nExamples of different timezone annotation for April 3, 2023. Standardized to ISO 8601\n\n\n\n\n\n\nDescription\nWritten in ISO 8601\n\n\n\n\nDate\n2023-04-03\n\n\nDate and Time with timezone offset\n2023-04-03T18:29:38+00:00\n\n\nDate and Time in UTC\n2023-04-03T18:29:38Z\n\n\nTime Interval in UTC (April 3 - 5, 2023)\n2023-04-03T18:29:38Z/2023-04-05T00:29:38Z\n\n\n\n\n\n\nhttps://imgs.xkcd.com/comics/iso_8601.png\n\n\n\n\nWhy?\n\nInternationally accepted format used across multiple schemas (e.g.¬†Darwin Core, EML, ISO 19115)\nRemoves ambiguity related to timezone, daylight savings time changes, and time of day\nBetter software integration of time date/time elements\n\n\n\nTop Resources\n\nISO 8601 wiki\nR package lubridate\nPython package datetime¬†\nArticle on datetime uncertainty\nMap of offset from UTC\nTime converter",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#match-scientific-names-to-a-taxonomic-authority",
    "href": "SOFTWARE_READY.html#match-scientific-names-to-a-taxonomic-authority",
    "title": "6¬† Make Your Data Software Ready",
    "section": "6.4 Match scientific names to a taxonomic authority",
    "text": "6.4 Match scientific names to a taxonomic authority\n\nWhat is it?\nA taxonomic authority is defined here as an online resource that maintains up-to-date species-level classification information and provides persistent identifiers (ID) for taxonomic classifications.\nExample: For the species Balaenoptera borealis (Lesson, 1828), the WoRMS taxonomic authority ID link is https://www.marinespecies.org/aphia.php?p=taxdetails&id=137088 and the Life Science Identifier (LSID) is urn:lsid:marinespecies.org:taxname:137088.\n\n\n\n\n\n\nSome important considerations\n\n\n\n\nConsider where you want to publish your data and use the existing taxonomic authority (e.g.¬†World Register of Marine Species, Integrated Taxonomic Information System, NCBI taxonomy) used in that repository\nInclude the authority who manages said information in your metadata.\nMake yourself aware of the structure, limits, and history of the authority you are using.\nAdopt standard binomial nomenclature, when possible.\nWhen possible, reference the unique identifier in addition to the nomenclature.\nIf possible, save and document the originally recorded name.\nPut notes about identification uncertainty in a separate column.\nMany authorities have APIs to facilitate matching names to identifiers.\n\n\n\n\n\nWhy?\n\nTo integrate or aggregate datasets, we need a common frame of reference for taxonomic name\nProvides an anchor for the taxonomy as scientific understanding evolves. We get more into that over in section 4.\n\n\n\nTop Resources\n\nGlobal Names Resolver allows users to compare taxonomic concepts across authorities\nList of authorities\nR packages\n\ntaxize is a taxonomic toolbelt for R, which wraps APIs for a large suite of taxonomic databases available on the web\nworrms and Worms are two API clients for World Register of Marine Species\nRitis is an API client for ITIS\n\nPython package WoRMS API client\nArticle: Recommendations for the Standardisation of Open Taxonomic Nomenclature for Image-Based Identifications\nTDWG 2022 Keynote: Richard Pyle, ‚ÄúAn Introduction to the Scientific Names of Organisms and the Taxon Concepts they Represent‚Äù",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#record-latitude-and-longitude-in-decimal-degrees-in-wgs84",
    "href": "SOFTWARE_READY.html#record-latitude-and-longitude-in-decimal-degrees-in-wgs84",
    "title": "6¬† Make Your Data Software Ready",
    "section": "6.5 Record latitude and longitude in decimal degrees in WGS84",
    "text": "6.5 Record latitude and longitude in decimal degrees in WGS84\n\nWhat is it?\nWGS84 is a coordinate reference system that clarifies location. Recording latitude and longitude coordinates in decimal degrees (DD), rather than degrees-minutes-seconds (DMS) or decimal-minutes (DM or DDM) standardizes them to be more machine and human readable. Degrees West and South are negative in decimal degrees, and longitude can range from -180 to 180, and longitude -90 to 90. Below are example coordinates in each format. Once locations are recorded in DD, the number of decimal places included should be adjusted to match the precision of the observation.\n\nExample Coordinates\n\n\nFormat\nExample\n\n\n\n\nDecimal Degrees (DD)\n30.50833333\n\n\nDegrees Minutes Seconds (DMS)\n30¬∞ 15‚Äô 10 N\n\n\nDegrees Decimal Minutes (DM or DDM)\n30¬∞ 15.1667 N\n\n\n\n\n\n\nhttps://imgs.xkcd.com/comics/coordinate_precision.png\n\n\n\n\nWhy?\n\nUsers have to know where you collected this data, which requires a latitude, longitude, reference system and uncertainty.\nDecimal-degrees avoids special symbols (¬∞ or ‚Äò) which is preferable for machine readable formats\nWGS84 is a reference coordinate system that is widely used and incorporated in many GPS units and tools, and recognized as a standard by many government agencies.\n\n\n\nTop Resources\n\nExisting R/python/ESRI packages/functions\n\nR package measurements¬†\nEML - find bounding coordinates\nCF coordinate conventions\n\nSome background on precision\nMore on precision\nDMS to DD calculator",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#use-persistent-unique-identifiers",
    "href": "SOFTWARE_READY.html#use-persistent-unique-identifiers",
    "title": "6¬† Make Your Data Software Ready",
    "section": "6.6 Use persistent unique identifiers",
    "text": "6.6 Use persistent unique identifiers\n\nWhat is it?\nPersistent unique identifiers (PIDs) are globally unique identifiers to unambiguously identify granules of information in a machine-readable way. Identifiers can exist in acquired data, or they may be created. When persistent unique identifiers from authorities exist, they should be used (e.g., when using a taxonomic authority like WoRMS). If PIDs are created, the user is responsible for managing them (e.g.¬†DOIs).\nPIDs can capture details about the underlying sampling event (e.g., the PID Station_95_Date_09JAN1997:14:35:00.000 is comprised of the time and place of sampling), or they can be opaque (i.e., not indicating anything about the content, e.g., the PID 10FC9784-B30F-48ED-8DB5-FF65A2A9934E), or semi-opaque. There are sometimes good reasons to keep an identifier opaque, but transparent or semi-opaque identifiers can guide humans as well as machines.\n\nExamples of PIDs\n\n\nType of PID\nUse Case\nExample\n\n\nDigital Object Identifier (DOI)\nActionable persistent link for papers, data, and other digital objects\nhttps://doi.org/10.6084/m9.figshare.16806712.v2\n\n\nInternational Geo Sample Number (IGSN)\nPersistent identifier for physical samples\nhttp://igsn.org/AU1243\n\n\nLife Science Identifier (LSID)\nPersistent structured method for biologically significant data\nurn:lsid:marinespecies.org:taxname:218214\n\n\nOpen Researcher and Contributor ID (ORCID)\nPersistent actionable link for individuals\nhttps://orcid.org/0000-0002-4391-107X\n\n\nResearch Organization Registry (ROR) Identifier\nPersistent actionable link for research organizations\nhttps://ror.org/01yvark48\n\n\n\n\n\nWhy?\n\nTo be able to uniquely identify a record in your data system or across data systems, it is important that it be persistent (consider samples possibly moving between institutions).\nAlthough it increases workload, it safeguards against confusion and inefficiency in the future.\nMaintains consistency through change, whether moving samplings between institutions, creating relational databases, or merging records.\nAllows users to precisely refer to data\n\n\n\nTop Resources\n\nA Beginner‚Äôs Guide to Persistent Identifiers\nSoftware and Packages to generate uuids:\n\nR package uuid¬†\nPython package uuid¬†\n[Generate, understand contruction, or parse existing GUIDs] (http://guid.one/)\n[Online GUID / UUID Generator] (https://guidgenerator.com/)\n\nGuidance on how to use GUIDs (Globally Unique Identifiers) to meet specific requirements of the biodiversity information community\nUse of globally unique identifiers (GUIDs) to link herbarium specimen records to physical specimens",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  }
]