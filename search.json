[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESIP Biological Data Cluster (BDS) Primer Guide",
    "section": "",
    "text": "1 Introduction\nThis is still a work in progress and only presented here for the purposes of receiving feedback. This message will be removed when the guidelines have been officially published.\nTo paraphrase the MARCO-BOLO data management plan, the main challenge with biological data is that it comes in many flavors. For example, it may include evidence of past or present organisms based on physical samples, chemicals, organic molecules, sound, or images. By using standards to organize and share data efficiently, biologists can turn this diversity of data types into a strength rather than an inherent challenge of dealing with biological data.\nThe ESIP Biological Data Standards Cluster formed in 2020 to maximize data relevance and utility for understanding changes in biodiversity over time. The cluster’s focus has expanded to include more aspects of biology beyond biodiversity, acknowledging that biological data may be useful for many facets of earth science. To accomplish this, the cluster encourages awareness and shared understanding of biological data standards by facilitating community building and information sharing via guidance, documentation, and training for the US biological data community. The first product from this cluster, the Biological Data Standards Primer, is an easy to digest resource for new biological data managers.\n\n\nWe build on this foundational product by providing additional context and details in the following suite of Biological Data Standards Primer Guides. The guides are intended to be a bridge between a deep dive into the standards documentation, and the lists of links available in the primer. The target audience for these guides is anyone interested in working with biological data, from data collection, to data sharing, and data management. We hope these Primer Guides will provide useful context for using biological standards.\nHow to contribute\nIf you would like to suggest changes or additions to the current version of the best practice documents, please use the GitHub issues to document your request. The current draft can be seen as a rendered webpage here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html",
    "href": "CONTEXT_UNDERSTANDABILITY.html",
    "title": "2  Provide Context and Understandability to Your Data",
    "section": "",
    "text": "2.1 Ecological Metadata Language (EML)\nMetadata standards ensure that data are described using a consistent structure and format to provide necessary context for users across the data lifecycle from data management to accessibility and interoperability. In biological data, there are a few important metadata standards to be aware of: the Ecological Metadata Language (EML), ISO-19115, and MIxS.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html#ecological-metadata-language-eml",
    "href": "CONTEXT_UNDERSTANDABILITY.html#ecological-metadata-language-eml",
    "title": "2  Provide Context and Understandability to Your Data",
    "section": "",
    "text": "What Is It?\nEcological Metadata Language (EML) is a metadata schema (i.e. standard) that was developed by the ecological community for ecological data, including biological data. Shared data can include an EML file that provides context for all files in the data “package”. EML is presented in Extensible Markup Language (XML), which provides standard details for ecological data in a structure that is readable to both people and machines.\nFor more information, see the full EML documentation or the National Center for Ecological Analysis and Synthesis (NCEAS)EML GitHub repository.\n\n\nWhy?\n\nIt provides context and improves reproducibility of the data.\nIt captures important links and relationships between data, such as a time series, hierarchical taxonomies, IDs, and authoritative material.\nEML helps represent ecological information in a standardized way.\nEML is mandatory for LTER, iLTER, OBIS, GBIF, Darwin Core Archive (DwC-A) data sharing.\n\n\n\nTop Resources\nTools or packages to help write EML:\n\nFor data managers, coders: \n\nEML-R package\nPostgresql database with fields compatible with EML\nR-code for generating EML from LTER-metabase (built on EML-R package)\nEMLAssemblyline (built on EML-R package)\n\nFor scientists or those not inclined to write scripts:\n\nezEML",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html#iso-19115",
    "href": "CONTEXT_UNDERSTANDABILITY.html#iso-19115",
    "title": "2  Provide Context and Understandability to Your Data",
    "section": "2.2 ISO 19115",
    "text": "2.2 ISO 19115\n\nWhat Is It?\nISO-19115 is a metadata standard, developed and maintained by the International Standards Organization (ISO), for describing geographic data. Biological data are inherently geographic, especially as we strive to understand how occurrences are impacted by ecological or environmental variables. ISO-19115 provides information about the identification, extent, quality, spatial and temporal schema, spatial reference, and distribution of geographic data. It evolved from the need for flexibility in harmonizing the Federal Geographic Data Committee (FGDC) Content Standard for Digital Geospatial Metadata (CSDGM) with other geospatial standards. For more information about implementations and extensions of ISO 19115, including for remotely sensed imagery and gridded data, see the Digital Curation Centre ISO 19115 guide or the NCEI Metadata Workbook.\n\n\nWhy?\n\nIt helps to provide important geographic context to data in a standardized way.\nUsing ISO metadata is mandatory for some U.S. federal agencies, like NOAA, NASA and USGS, to share their data through government repositories.\nIt can be used to describe individual files, data packages, and collections of datasets.\n\n\n\nTop Resources\n\nHow to Convert ISO to EML \nWork Flow Model\nmdToolkit - mdEditor is a writer for ISO 19115 metadata which uses mdJSON as an intermediary and mdTranslator allows translation to different metadata formats",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html#minimum-information-about-any-x-sequence-mixs",
    "href": "CONTEXT_UNDERSTANDABILITY.html#minimum-information-about-any-x-sequence-mixs",
    "title": "2  Provide Context and Understandability to Your Data",
    "section": "2.3 Minimum Information about any (x) Sequence (MIxS)",
    "text": "2.3 Minimum Information about any (x) Sequence (MIxS)\n\nWhat Is It?\nMIxS (pronounced MIX-ess) is a set of checklists and packages for molecular genomic sequence data, such as DNA and RNA. MIxS is a standard published by the Genome Standards Consortium (GSC) for molecular biologists and ecologists who create, manage, and archive sequence data. It includes a breadth of environment-specific metadata variables (e.g. soil variables) to augment genome-specific checklists (e.g. bacteria) and enables interoperability with environmental analyses.\n\n\nWhy?\n\nIt helps to provide minimal standardized metadata about genetic sequence data.\nIt is used by the International Nucleotide Sequence Database Collection (INSDC), which has the following member participating databases: ROIS - NIG, EMBL-EBI and NCBI.\n\n\n\nTop References\n\nMIxS Term Search Tool\nGenomic Standards Consortium term list\nMinimum Information about Marker Gene Sequence (MIMARKS)\nMIxS is maintained by the community using GitHub. To propose changes, ask questions, see the MIxS GitHub repository.\nMinimum Information about Sequence Data from the Built Environment (MIxS-BE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "INTEGRATE_DATA.html",
    "href": "INTEGRATE_DATA.html",
    "title": "3  Integrate Your Data with Other Data",
    "section": "",
    "text": "3.1 Darwin Core\nBiodiversity data are collected and managed in many different systems and environments from museum collections, to environmental monitoring programs, research programs, or community science projects (e.g. iNaturalist). Data are often heterogeneous within and across these systems, depending on research objectives, but there are consistent themes in them: the what, where, and when. Although the details may have names unique to your own data, there are specific field names to use to provide these details in a way that aligns with others’ data. Through the use of common terminology, downstream users can not only better search for and discover data, but also evaluate, integrate, and compare datasets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrate Your Data with Other Data</span>"
    ]
  },
  {
    "objectID": "INTEGRATE_DATA.html#darwin-core",
    "href": "INTEGRATE_DATA.html#darwin-core",
    "title": "3  Integrate Your Data with Other Data",
    "section": "",
    "text": "What Is It: \nDarwin Core (DwC) is a data standard that offers a stable, simple, and flexible framework for compilation and reuse of biodiversity data, including observations, specimens, samples, and related information, from varied and variable sources. DwC builds upon Dublin Core, a set of metadata terms used by libraries to describe physical and digital resources, to describe biological occurrences. The DwC glossary of terms provides identifiers, labels, and definitions to map occurrence information from multiple sources in a cohesive and interpretable way. \nA single dataset, known as the Darwin Core Archive (DwC-A), is a compressed (.zip) file that contains interconnected text files (e.g. csv or tsv) with DarwinCore standard-mapped data that are arranged into core files. To facilitate human- and machine-interpretation of the data, .xml files are included to describe the contents of the Archive, the relationships between the core files and connected tables contained in the DwC-A.\n\n\nWhy?\n\nTaxonomic occurrence data can be standardized to Darwin Core irrespective of the observing method by which the data were collected (e.g., observational data, genomics, imaging, animal tracking). \nThe Darwin Core standard plays a fundamental role in facilitating open-access biodiversity data sharing, use, and reuse. \nAligning your data to DwC facilitates the use of heterogeneous biological data gathered using disparate collection methods.\nThe Global Biodiversity Information Facility (GBIF),Ocean Biodiversity Information System (OBIS), andThe Atlas of Living Australia (ALA), andmany more repositoriesuse this data standard.\n\n\n\nTop Resources\n\nDarwin Core Quick Reference Guide.\nWieczorek et al., (2012) Darwin Core: An Evolving Community-Developed Biodiversity Data Standard.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrate Your Data with Other Data</span>"
    ]
  },
  {
    "objectID": "INTEGRATE_DATA.html#climate-and-forecast-cf",
    "href": "INTEGRATE_DATA.html#climate-and-forecast-cf",
    "title": "3  Integrate Your Data with Other Data",
    "section": "3.2 Climate and Forecast (CF)",
    "text": "3.2 Climate and Forecast (CF)\n\nWhat Is It:\nThe Climate and Forecast (CF) Metadata Conventions provide a framework to clarify details about the context of each piece of data: a description of what the data in each variable represents and their spatial and temporal properties. Although CF is well-established, it is imperfect for biological data, but terms from CF are often applicable in biological data. CF units are specified using the UDUNITS system. CF conventions are built on top of the netCDF (Network Common Data Form) standard and promote the processing and sharing of netCDF files created with the NetCDF API, so standards like COARDS conventions are applied.\nFor more information, see overviews of CF as a presentation and paper.\n\n\nWhy?\n\nCF enables users of data from different sources to decide which quantities are comparable.\nIt facilitates building applications with powerful extraction, regridding, and display capabilities. \nA CF principle is self-contained, meaning as general and well-defined as possible, so the reader does not have to access outside sources to understand the terms.\nThere are many efforts that use CF, including the National Centers for Environmental Information (NCEI) and NASA EarthData repositories. For a longer list, please see here.\nThe NERC Vocabulary Server hosts CF and maps it to other vocabularies. \n\n\n\nTop Resources\n\nTo find standard names that describe your data, open up the latest Standard Name table (as HTML or XML) and search through it for words typically used for your data.\nCF is developed and maintained by the community using GitHub. To propose changes, ask questions, see the CF GitHub repositories.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrate Your Data with Other Data</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html",
    "href": "INTEROPERABLE_DATA.html",
    "title": "4  Make Your Data Interoperable",
    "section": "",
    "text": "4.1 Catalogue of Life (COL)\nControlled vocabularies can provide standardized terms and definitions for biological concepts, like biological taxonomies. Using a controlled vocabulary facilitates the integration and sharing of data across diverse sources, improving consistency and data quality, for example, in species classification.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#catalogue-of-life-col",
    "href": "INTEROPERABLE_DATA.html#catalogue-of-life-col",
    "title": "4  Make Your Data Interoperable",
    "section": "",
    "text": "What Is It?\nCatalogue of Life describes itself as, “The most complete authoritative list of the world’s species - maintained by hundreds of global taxonomists.” It brings together information from taxonomists, and taxonomic databases, to construct an integrated view of currently accepted species across all taxonomic groups. A list of source datasets can be found here.\n\nThe primary mission of COL is to deliver data, but the tools and services offered by COL also enable taxonomists and other stakeholders to publish and revise species lists for any purpose.\n\n\nWhy?\n\nCOL adds persistent identifiers that enable users to track changes to a scientific name. \nCOL helps downstream users consider the most up-to-date past and current characteristics of an organism: its biology, distribution, relevance to humans, and evolutionary history.\n\n\n\nTop Resources\n\nUsers can browse the COL Checklist, which is updated monthly. COL pulls information from specific data sources, e.g. FishBase (see: https://www.catalogueoflife.org/data/taxon/49JFH). \nCOL data pipeline for COL taxonomic checklist data: https://www.catalogueoflife.org/about/colpipeline. \nCOL ChecklistBank API: https://api.checklistbank.org/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#integrated-taxonomic-information-system-itis",
    "href": "INTEROPERABLE_DATA.html#integrated-taxonomic-information-system-itis",
    "title": "4  Make Your Data Interoperable",
    "section": "4.2 Integrated Taxonomic Information System (ITIS)",
    "text": "4.2 Integrated Taxonomic Information System (ITIS)\n\nWhat Is It?\nITIS is a taxonomic database, developed and maintained by a partnership of federal agencies that provides reliable information on the nomenclature, taxonomy, and distribution of 1.8 million species of plants, animals, fungi and microbes in North America and the world. ITIS couples each scientific name with a unique taxonomic serial number (TSN), which ensures consistency and accuracy in the naming and classification of species.\n\n\nWhy?\n\nUnique taxonomic serial number (TSN) ensures consistent and accurate naming and classification of species\nITIS is an important tool for identifying and cataloging species and monitoring their populations. \n\n\n\nTop Resources\n\nITIS website \nITIS API",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#paleobiology-database-pbdb",
    "href": "INTEROPERABLE_DATA.html#paleobiology-database-pbdb",
    "title": "4  Make Your Data Interoperable",
    "section": "4.3 Paleobiology Database (PBDB)",
    "text": "4.3 Paleobiology Database (PBDB)\n\nWhat Is It?\nThe Paleobiology Database (PBDB) is an online, expert-curated database that aims to provide taxonomic information for paleobiological taxa of all geological ages. It contains data for almost half a million paleobiological taxa from over 900 different contributors.\n\n\nWhy?\n\nChecking your paleobiological taxonomic names against the PBDB will ensure the names are up-to-date based on current taxonomic literature as reflected in the database.\nPBDB provides the taxonomic backbone to the Global Biodiversity Information Facility (GBIF). This means aligning your taxonomic names with PBDB will make the process of sharing your data easier.\n\n\n\nResources\n\nPBDB can be accessed via their website, mobile applications, and an API.\nThe PBDB website has a Resources tab where more information about these access points can be found. \nThe same Resources page also includes information on how to contribute taxonomic information to PBDB. Lessons are available via a playlist of YouTube videos. Note that you will need to be granted Contributory User access to contribute to PBDB. More information about the process and different user types is available in the PBDB User Guide.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#world-register-of-marine-species-worms",
    "href": "INTEROPERABLE_DATA.html#world-register-of-marine-species-worms",
    "title": "4  Make Your Data Interoperable",
    "section": "4.4 World Register of Marine Species (WoRMS)",
    "text": "4.4 World Register of Marine Species (WoRMS)\n\nWhat Is It?\nThe World Register of Marine Species (WoRMS) is an authoritative and comprehensive list of names of marine organisms. It provides detailed information about marine species from around the world, helping anyone interested in marine life to find accurate and up-to-date information about these species, including where they can be found, their characteristics, and how they are related to one another. \n\n\nWhy?\n\nWoRMS content is curated by taxonomic and thematic experts, not by database managers. \nEach taxonomic group is represented by an expert who has the authority over the content, and is responsible for controlling the quality of the information. Each of these main taxonomic editors can invite several specialists of smaller groups within their area of responsibility to join them. \nWoRMS is the taxonomic database used by OBIS, and other important biological initiatives.\n\n\n\nTop Resources\n\nWoRMS and its associated tools can be explored via web browser or the WoRMS API\nR packages: worrms, taxize\nPython package",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html",
    "title": "5  Make Your Data Internet Ready",
    "section": "",
    "text": "5.1 Web-enabled standards\nWeb services and standards are useful to understand if you collect or manage biological data. Platforms like NOAA’s NCEI, NASA, OBIS, and GBIF, etc. utilize standard web services to serve data. Web-friendly data standards facilitate the transfer and handling of data via web services by making information visible in a predictable way, promoting online sharing, programmatic discovery, access, and processing of data across platforms and disciplines.\nWeb standards are the formal, non-proprietary standards and other technical specifications that define and describe different aspects of the World Wide Web. Web standards are created by standards bodies, which are institutions that invite groups of people to come together and agree on how the technologies should work in the best way to fulfill specific use cases. Web standards are key to global data discovery.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#web-enabled-standards",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#web-enabled-standards",
    "title": "5  Make Your Data Internet Ready",
    "section": "",
    "text": "5.1.1 W3C standards\n\n\nWhat Is It?\nThe World Wide Web Consortium (W3C) develops the W3C standards, which serve as building blocks to build internet browsers, web pages, blogs, search engines, and other software that power our experience on the web. Although HTML is its cornerstone, W3C publishes a range of technical reports, which help move the web forward, like CSS, SVG, WOFF, WebRTC, XML, and a growing variety of APIs.\n\n\nWhy?\n\nDevelopers can create interactive experiences available on any device.\nData can be made more FAIR by increasing your awareness of these standards.\n\n\n\nTop Resources\n\nW3Schools offers a variety of tutorials for free",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#section",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#section",
    "title": "5  Make Your Data Internet Ready",
    "section": "5.2 ",
    "text": "5.2 \n\n5.1.2 Dublin Core Standard\n\n\nWhat Is It?\nDublin Core is a metadata standard of 15 ‘core’ terms originally developed for archives and libraries to describe physical or digital resources and details about their collection. Darwin Core is an extension of Dublin Core for biodiversity information.\n\n\nWhy?\n\nCrowd source: Why is it beneficial to know about Dublin Core for internet ready data?\n\n\n\nTop Resources\n\nDisambiguating the Cores presentation\nDublin Core Metadata Initiative documentation\n\n\n\n5.1.3 DataCite\n\n\nWhat Is It?\nThe DataCite metadata schema is an international, not-for-profit organization which aims to improve data citation through web-enabled standards that connect products and citations. \n\n\nWhy?\n\nHelps mint persistent identifiers, such as digital object identifiers (DOI), for research products, which enables data archiving and long-term preservation\nHelps connect the research product to researchers through other persistent identifiers, such as ORCIDs for researchers or ROR for organizations\nPromotes long-term preservation, accessibility, reuse, and attribution of research products with citable contributions to a scholarly record\nConnects users and publishing machinery\n\n\n\nTop Resources\n\nREST API, which enables retrieval, creation, and update of a DOI metadata record.\nAdditional documentation on DataCite\n\n\n\n5.1.4 Schema.org\n\n\nWhat Is It?\nSchema.org provides documentation on a set of extensible schemas, which are schemas where users can use components to create other schemas. This enables users to embed structured data on their web pages to help search engines understand the information presented and provide richer search results. Using schema.org vocabulary as well as various formats (e.g., JSON-LD) to mark up website content with metadata about itself, makes it easier for websites or data records to not only be searched but also for the relationships between them to be understood.\n\n\nWhy?\n\nMake your research more easily and prominently discvoerable through major search engines.\n\n\n\nTop Resources\n\nYou can add schema.org markup to your webpages or records using various online tools, including Google’s Structured Data Markup Helper, or by directly adding code to your webpages. \nDocumentation on schema.org",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#web-services",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#web-services",
    "title": "5  Make Your Data Internet Ready",
    "section": "5.3 Web Services",
    "text": "5.3 Web Services\nWeb services run much of our digital world today. You probably use them through your phone every day, without noticing a thing. You can think of a web service as a waiter at a restaurant. You (the user) order food (a request), the waiter (the web service) takes your order to the kitchen (the server or application), and then brings you back your food (the response). This allows different parts of a computer system or different systems altogether to interact without needing to know how each other works internally. When web services are fully utilized, it results in impressive hi-speed analysis, like the analytics shared during football (all types 🙂) games, the olympics, and other sporting events.\n\n5.2.1 ERDDAP™ Web Service\n\n\nWhat Is It?\nERDDAP™ (pronounced ur-dap) is a data server that offers users a simple and consistent way to download, integrate, analyze, visualize, and map multiple scientific datasets from different sources and scientific communities – typically oceanographic and atmospheric data.\nTo facilitate comparisons of data from different datasets, requests and results in ERDDAP™ use standardized space/time axis, which makes it easier for users to specify data constraints in requests without having to worry about the data format. ERDDAP™ allows users to request a subset of a dataset, and can convert the subset to a desired file format such as .csv, .json, .nc and others, for download.\n\n\nWhy?\n\nERDDAP™ is free, open source, and used globally\nAll information, data, and figures made available via ERDDAP™ are also available via an API, making data programmatically accessible.\nERDDAP™ has a RESTful web service which is designed to be easy for computer programs and scripts to use or interact with.\nUsed for oceanographic and atmospheric datasets, but also works great for biological and biodiversity-relevant observations\nGood for both gridded and tabular data - See table dataset API docs here, and for gridded datasets here.\n\n\n\nTop Resources\n\nCoastWatch Training and specifically ERDDAP basics\nAwesome ERDDAP\nOverview: Distributed Model Data Access\nData providers can set up their own ERDDAP server to serve up their data.\nAdditional overall documentation on ERDDAP can be found here.  \n\n\n\n5.2.2 Thematic Real-time Environmental Distributed Data Services (THREDDS)\n\n\nWhat Is It?\nThe THREDDS server, which was developed prior to ERDDAP, has features and interfaces that makes it easier to explore and use data. \n\n\nWhy?\n\nCrowd source: What is beneficial about THREDDS for internet ready data?\n\n\n\nTop Resources\n\nA comparison of ERDDAP and THREDDS\n\n\n\n5.2.3 Web Map Service\n\n\nWhat Is It?\nA Web Map Service (WMS) is a way to retrieve georegistered map images over the internet to display in applications and web pages. The WMS specifications were developed by the Open Geospatial Consortium (OGC) to enable interoperability and use in web browsers, open-source GIS software (ex. QGIS), and proprietary GIS software (ex. Esri).\n\n\nWhy?\n\nWMS allows you to view and use maps from different sources that host the maps and data used to create them without needing to download them.\n\n\n\nTop Resources\n\nCrowd source: What is beneficial about Web Map Services for internet ready data?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html",
    "href": "SOFTWARE_READY.html",
    "title": "6  Make Your Data Software Ready",
    "section": "",
    "text": "6.1 Use non-proprietary formats\nThose sharing or managing data can take small steps to make them “software ready.” These include using non-proprietary formats, structuring tables with specific columns and entries, including standards for information about time, place, and organism.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#use-non-proprietary-formats",
    "href": "SOFTWARE_READY.html#use-non-proprietary-formats",
    "title": "6  Make Your Data Software Ready",
    "section": "",
    "text": "What is it?\nNon-proprietary file formats do not require specific software and can be accessed without licenses and within different software systems.For example, comma separated values (CSV) format is becoming an increasingly popular non-proprietary format compared to the proprietary .xlsx format.\n\n\nWhy?\n\nAllows data to be useful in perpetuity by ensuring data readability and reusability across multiple platforms\nAligns better with the FAIR data principles\nMakes data more socially equitable, supporting open science\nMany applications (e.g. Microsoft Office) allow exporting into multiple formats, which makes it easy to share data in non-proprietary formats even if it was created using proprietary software.\n\n\n\nTop Resources\n\nTable of commonly used formats for common data types\nA more detailed table that is specific to U.S. Federal records management",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#structure-tabular-data-in-tidylong-format",
    "href": "SOFTWARE_READY.html#structure-tabular-data-in-tidylong-format",
    "title": "6  Make Your Data Software Ready",
    "section": "6.2 Structure tabular data in tidy/long format",
    "text": "6.2 Structure tabular data in tidy/long format\n\nWhat is it?\nLong (or sometimes called “tidy”) format for tabular data can best be described as having one observation per row.\nThe following example shows two different formats – wide and long – of the same data. Notice that while sites 1, 2, and 3 are the column names filled with counts for each species in the wide format, site and count become the column names in long format.\n\n\nWhy?\n\nThe clear structure makes data more machine readable, particularly with commonly-used analytical software.\nData are as atomic as possible (e.g. no mixed types in one field)\nIt is easier to aggregate data across multiple files\n\n\n\n\nExample of Wide Format\n\n\nspeciessite_01site_02site_03Tilia americana450Pinus strobus331\n\n\nExample of Long Format\n\n\nspeciessitecountTilia americanasite_010Tilia americanasite_021Tilia americanasite_033Pinus strobussite_011Pinus strobussite_022Pinus strobussite_035\n\n\n\n\nTop Resources\n\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23.\nVideo: Data Sharing and Management Snafu in 3 Short Acts (video)\nTips for working with data in BASH\nData Organization in Spreadsheets for Ecologists\nCleaning Data and Quality Control",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#follow-iso-8601-for-dates",
    "href": "SOFTWARE_READY.html#follow-iso-8601-for-dates",
    "title": "6  Make Your Data Software Ready",
    "section": "6.3 Follow ISO 8601 for dates",
    "text": "6.3 Follow ISO 8601 for dates\n\nWhat is it?\nISO 8601 is a convention for dates and times, where dates are listed as YYYY-MM-DD and time is given in Coordinated Universal Time (UTC, Zulu, or GMT) which is the time standard, relative to 0o longitude, that regulates global clocks.\nThe following table outlines how to write dates, times, and time intervals using ISO 8601:\nExamples: April 3, 2023 standardized to ISO 8601\n\n\nDescriptionWritten in ISO 8601Date2023-04-03Date and Time with timezone offset2023-04-03T18:29:38+00:00Date and Time in UTC2023-04-03T18:29:38ZTime Interval in UTC (April 3 - 5, 2023)2023-04-03T18:29:38Z/2023-04-05T00:29:38Z\n\n\nExamples: different styles of timezone annotation\n\n\nDescriptionWritten in ISO 8601Date2023-04-03Date and Time with timezone offset2023-04-03T18:29:38+00:00Date and Time in UTC2023-04-03T18:29:38ZTime Interval in UTC (April 3 - 5, 2023)2023-04-03T18:29:38Z/2023-04-05T00:29:38Z\n\n\n\n\n\n\n\n\nhttps://imgs.xkcd.com/comics/iso_8601.png\n\n\n\n\nWhy?\n\nInternationally accepted format used across multiple schemas (e.g. Darwin Core, EML, ISO 19115)\nRemoves ambiguity related to timezone, daylight savings time changes, and time of day\nBetter software integration of time date/time elements\n\n\n\nTop References\n\nISO 8601 wiki\nR package lubridate\nPython package datetime \nArticle on datetime uncertainty\nMap of offset from UTC\nTime converter",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#match-scientific-names-to-a-taxonomic-authority",
    "href": "SOFTWARE_READY.html#match-scientific-names-to-a-taxonomic-authority",
    "title": "6  Make Your Data Software Ready",
    "section": "6.4 Match scientific names to a taxonomic authority",
    "text": "6.4 Match scientific names to a taxonomic authority\n\nWhat is it?\nA taxonomic authority is defined here as an online resource that maintains up-to-date species-level classification information and provides persistent identifiers (ID) for taxonomic classifications.\nExample: For the species Balaenoptera borealis (Lesson, 1828), the WoRMS taxonomic authority ID link is https://www.marinespecies.org/aphia.php?p=taxdetails&id=137088 and the Life Science Identifier (LSID) is urn:lsid:marinespecies.org:taxname:137088.\nSome important considerations:\n\nConsider where you want to publish your data and use the existing taxonomic authority (e.g. World Register of Marine Species, Integrated Taxonomic Information System, NCBI taxonomy) used in that repository\nInclude the authority who manages said information in your metadata.\nMake yourself aware of the structure, limits, and history of the authority you are using.\nAdopt standard binomial nomenclature, when possible.\nWhen possible, reference the unique identifier in addition to the nomenclature.\nAlways save and document the originally recorded name.\nPut notes about identification uncertainty in a separate column.\nMany authorities have APIs to facilitate matching names to identifiers.\n\n\n\nWhy?\n\nTo integrate or aggregate datasets, we need a common frame of reference for taxonomic name\nProvides an anchor for the taxonomy as scientific understanding evolves.\n\n\n\nTop Resources\n\nGlobal Names Resolver allows users to compare taxonomic concepts across authorities\nList of authorities\nR packages\n\ntaxize is a taxonomic toolbelt for R, which wraps APIs for a large suite of taxonomic databases available on the web\nworrms and Worms are two API clients for World Register of Marine Species\nRitis is an API client for ITIS\n\nPython package WoRMS API client\nArticle: Recommendations for the Standardisation of Open Taxonomic Nomenclature for Image-Based Identifications\nTDWG 2022 Keynote: Richard Pyle, “An Introduction to the Scientific Names of Organisms and the Taxon Concepts they Represent”",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#record-latitude-and-longitude-in-decimal-degrees-in-wgs84",
    "href": "SOFTWARE_READY.html#record-latitude-and-longitude-in-decimal-degrees-in-wgs84",
    "title": "6  Make Your Data Software Ready",
    "section": "6.5 Record latitude and longitude in decimal degrees in WGS84",
    "text": "6.5 Record latitude and longitude in decimal degrees in WGS84\n\nWhat is it?\nWGS84 is a coordinate reference system that clarifies location. Recording latitude and longitude coordinates in decimal degrees (DD), rather than degrees-minutes-seconds (DMS) or decimal-minutes (DM or DDM) standardizes them to be more machine and human readable. Degrees West and South are negative in decimal degrees, and longitude can range from -180 to 180, and longitude -90 to 90. Below are example coordinates in each format. Once locations are recorded in DD, the number of decimal places included should be adjusted to match the precision of the observation.\nExample Coordinates\n\n\nFormatExampleDecimal Degrees (DD)30.50833333Degrees Minutes Seconds (DMS)30° 15' 10 NDegrees Decimal Minutes (DM or DDM)30° 15.1667 N\n\n\n\n\n\nhttps://imgs.xkcd.com/comics/coordinate_precision.png\n\n\n\n\nWhy?\n\nUsers have to know where you collected this data, which requires a latitude, longitude, reference system and uncertainty.\nDecimal-degrees avoids special symbols (° or ‘) which is preferable for machine readable formats\nWGS84 is a reference coordinate system that is widely used and incorporated in many GPS units and tools, and recognized as a standard by many government agencies.\n\n\n\nTop Resources\n\nExisting R/python/ESRI packages/functions\n\nR package measurements \nEML - find bounding coordinates\nCF coordinate conventions\n\nSome background on precision\nMore on precision\nDMS to DD calculator",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#use-persistent-unique-identifiers",
    "href": "SOFTWARE_READY.html#use-persistent-unique-identifiers",
    "title": "6  Make Your Data Software Ready",
    "section": "6.6 Use persistent unique identifiers",
    "text": "6.6 Use persistent unique identifiers\n\nWhy?\n\nIt can be useful to have unique identifiers to unambiguously identify granules of information, e.g. dataset, collection, database, taxonomic concept, etc. This will allow users to precisely refer to the data and allow your data to remain identifiable when aggregated with other datasets.\nTo be able to uniquely identify a record in your data system or across data systems. Useful to create relational databases or merge records.\nAlthough it increases workload, it safeguards against confusion and inefficiency in the future.\n\n\n\nKey Information\n\nThere are good reasons to keep an identifier opaque, i.e. it does not indicate anything about the content of information it points to. However, there are also transparent, or semi-opaque identifiers in use that take advantage of semantics to guide humans as well as machines.\nOne way to create a unique identifier is concatenation of sampling event, location, time, enumeration of unique observation or event. (e.g. Station_95_Date_09JAN1997:14:35:00.000)\nSome prefer using opaque identifiers. (e.g. 10FC9784-B30F-48ED-8DB5-FF65A2A9934E)\nIf there is an existing persistent unique identifier, it’s usually a good idea to use it (i.e. when using a taxonomic authority like WoRMS and applying their LSID).\nIt is important to manage any identifiers you create, if they are not managed by an authority (e.g. DOIs).\nImportant that it be persistent (consider samples possibly moving between institutions)\n\nExamples of PIDs\n\n\nType of PIDUse CaseExampleDigital Object Identifier (DOI)Actionable persistent link for papers, data, and other digital objectshttps://doi.org/10.6084/m9.figshare.16806712.v2International Geo Sample Number (IGSN)Persistent identifier for physical sampleshttp://igsn.org/AU1243&gt;Life Science Identifier (LSID)Persistent structured method for biologically significant dataurn:lsid:marinespecies.org:taxname:218214Open Researcher and Contributor ID (ORCID)Persistent actionable link for individualshttps://orcid.org/0000-0002-4391-107X\n\n\n\n\nTop References\n\nSoftware and Packages to generate uuids:\n\nR - uuid https://cran.r-project.org/web/packages/uuid/index.html\npython - uuid https://docs.python.org/3/library/uuid.html\nhttp://guid.one/\nhttps://guidgenerator.com/\n\nGuidance on how to use GUIDs (Globally Unique Identifiers) to meet specific requirements of the biodiversity information community\nhttp://bioimages.vanderbilt.edu/pages/guid-applicability-final-2011-01.pdf\nUse of globally unique identifiers (GUIDs) to link herbarium specimen records to physical specimens\nhttps://bsapubs.onlinelibrary.wiley.com/doi/full/10.1002/aps3.1027\nA Beginner’s Guide to Persistent Identifiers\nhttp://links.gbif.org/persistent_identifiers_guide_en_v1.pdf",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  }
]