[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESIP Biological Data Cluster (BDS) Primer Guide",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#esip-bds-primer-guide-suite-of-documents",
    "href": "index.html#esip-bds-primer-guide-suite-of-documents",
    "title": "ESIP Biological Data Cluster (BDS) Primer Guide",
    "section": "ESIP BDS Primer Guide Suite of Documents",
    "text": "ESIP BDS Primer Guide Suite of Documents\nThe ESIP Biological Data Standards Cluster formed in 2020 to maximize data relevance and utility for understanding changes in biodiversity over time. To accomplish this the cluster facilitates guidance, best practice documentation, training, and community building for the US biological data community. The first product from this cluster Biological Data Standards Primer, while an easy to digest resource, does not provide the context data managers need to decide which standards to use for the data they are working with. The guides are intended to be a bridge between the full, lengthy standards documentation, and the short primer quick reference. The first document being developed is for the “Make Your Data Software Ready?” section of the primer.\n\nHow to contribute\nIf you would like to suggest changes or additions to the current version of the best practice documents, please use the GitHub issues to document your request. The current draft can be seen as a rendered webpage here.\n\n\nStructure\nThe structure for each section is:\n\nValue proposition (Why?)\nList / key information (bulleted)\nReferences list",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Test\nThis is still a work in progress and only presented here for the purposes of receiving feedback. This message will be removed when the guidelines have been officially published.\nThe diversity of biological data, and (seeming) lack of overarching community standards makes working with biological data challenging. Several standards do exist for biological data, however these different data, metadata, and taxonomic standards are confusing for data managers and data users to navigate. The biological data community in the US could benefit from guidance, best practice documentation, training, and community building. The ESIP Biological Data Cluster (BDS) was formed to tackle these problems.\nIn short, these guidelines will be successful if they:\n\nIncrease awareness & interest in biological standards\nCreate unity/shared vision around biological data standards implementation\nProvide guidance related to biological standards\nProvide opportunities for knowledge sharing & coordination\nProvide connectivity across ESIP\n\nWe successfully increased awareness and interest in standards through the creation of the primer, officially called, Biological Observation Data Standardization - A Primer for Data Manager. \nOur goals have evolved to work toward creating a sample extension of the cluster’s primer in the form of best practice documents. The target audience for the best practice documents are people who are new to working with biological data standards. The best practice document will provide additional detail on the points listed in the primer, focusing on efficient actions that can be taken to standardize data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html",
    "href": "CONTEXT_UNDERSTANDABILITY.html",
    "title": "2  Provide Context and Understandability to Your Data",
    "section": "",
    "text": "2.1 Ecological Metadata Language (EML)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html#ecological-metadata-language-eml",
    "href": "CONTEXT_UNDERSTANDABILITY.html#ecological-metadata-language-eml",
    "title": "2  Provide Context and Understandability to Your Data",
    "section": "",
    "text": "What Is It?\nEML is a community-developed metadata schema designed for ecological data, which encompasses biological data. EML is normally presented as Extensible Markup Language (XML). An EML instance (XML document) holds metadata to describe one or more data objects. Data tables are the most common, but almost any data object can be accommodated.\n\n\nWhy?\n\nProvide context to your data and improve reproducibility of the data\nCan capture linked data relationships within EML (dataset series)\nStandardized representation of information\nEML was designed for ecological data, which encompasses biological data\nIt’s taxonomic fields cover relationships (hierarchies), IDs, and authoritative material\n\n\n\nKey Information\n\nEML Schema\nMandatory for LTER, iLTER, OBIS, GBIF, Darwin Core Archive (DwC-A)\nMaintained and managed through a GitHub repository by NCEAS\nUsually, what you would submit to a repository is a “data package” consisting of an EML document and one or more data objects.\n\n\n\nTop References\nTools or packages to help write EML:\n\nFor data managers, coders: \n\nEML-R package\nPostgresql database with fields compatible with EML\nR-code for generating EML from LTER-metabase (built on EML-R package)\nEMLAssemblyline (built on EML-R package)\n\nFor scientists or those not inclined to write scripts\n\nezEML",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html#iso-19115",
    "href": "CONTEXT_UNDERSTANDABILITY.html#iso-19115",
    "title": "2  Provide Context and Understandability to Your Data",
    "section": "2.2 ISO 19115",
    "text": "2.2 ISO 19115\n\nWhat Is It?\nContent standard for describing geographic data sponsored by the International Standards Organization (ISO). At its most basic, it is written in narrative form with class diagrams. There are many implementations and extensions (e.g., https://www.dcc.ac.uk/resources/metadata-standards/iso-19115).\n\n\nWhy?\n\nProvide context to your data (biological data is inherently ‘geographic’)\nStandardized representation of information\nMandated by some US federal agencies, including NOAA, NASA, and USGS\nCan be used at different granularities, used to describe data packages or collections, as well as at a dataset level (?): content standard vs collection standard?\n\n\n\nWhat?\n\nEvolved from the need to harmonize the FGDC Content Standard for Digital Geospatial Metadata (CSDGM) with other formal and defacto standards that support the documentation of geospatial data and services.\nMany variations including 19115, 19115-1, 19115-2. From NCEI:\n\nISO 19115 Geographic information – Metadata: The ISO standard for documenting geospatial data. \nISO 19115-2 Geographic information – Metadata – Part 2: Extensions for imagery and gridded data: An extension of ISO 19115 used to document information about imagery, gridded data, and remotely sensed data. The root of ISO 19115 metadata records will change from MD_Metadata to MI_Metadata when using ISO 19115-2.\n\nUsurped FGDC CSDGM - all users encouraged to migrate to ISO.\nHighly flexible for many uses compared FGDC CSDGM, but few required elements leaves room for incomplete metadata\n\n\n\nTop References\n\nNOAA Workbook for ISO 19115-2 \nHow to Convert ISO to EML \nWork Flow Model\nmdToolkit - mdEditor is a writer for ISO 19115 metadata which uses mdJSON as an intermediary and mdTranslator allows translation to different metadata formats",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "CONTEXT_UNDERSTANDABILITY.html#minimum-information-about-any-x-sequence-mixs",
    "href": "CONTEXT_UNDERSTANDABILITY.html#minimum-information-about-any-x-sequence-mixs",
    "title": "2  Provide Context and Understandability to Your Data",
    "section": "2.3 Minimum Information about any (x) Sequence (MIxS)",
    "text": "2.3 Minimum Information about any (x) Sequence (MIxS)\n\nWho?\nThis is a standard for molecular data, like DNA and RNA. It is used by molecular biologists and ecologists who generate, manage and archive these type of sequence data.\n\n\nWhat Is It?\nA set of checklists and packages for genomic sequence data.\n\n\nWhy?\n\nProvide minimal standardized metadata about genetic sequence data\nAgreed upon and published by the Genome Standards Consortium\nUsed by the INSDC (DDBJ, EMBL-EBI and NCBI)\n\n\n\nKey Information\n\nMIxS (pronounced MIX-ess), a suite of checklists standards, is introduced to report on a breadth of environment-specific metadata variables to augment the genome-specific checklists\nEnables mixing and matching of genome checklists and environmental-specific packages\n\n\n\nMIxS Structure\n\n\n\n\n\nTop References\n\nMIxS Term Search Tool\nGenomic Standards Consortium term list\nMinimum Information about Marker Gene Sequence (MIMARKS)\nMIxS GitHub repo\nMinimum Information about Sequence Data from the Built Environment (MIxS-BE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Provide Context and Understandability to Your Data</span>"
    ]
  },
  {
    "objectID": "INTEGRATE_DATA.html",
    "href": "INTEGRATE_DATA.html",
    "title": "3  Integrate Your Data with Other Data",
    "section": "",
    "text": "3.1 Climate and Forecast (CF)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrate Your Data with Other Data</span>"
    ]
  },
  {
    "objectID": "INTEGRATE_DATA.html#climate-and-forecast-cf",
    "href": "INTEGRATE_DATA.html#climate-and-forecast-cf",
    "title": "3  Integrate Your Data with Other Data",
    "section": "",
    "text": "What Is It: \nThe Climate and Forecast (CF) metadata conventions are designed to promote the processing and sharing of files created with the NetCDF (Network Common Data Form) API. The conventions define metadata that provide a definitive description of what the data in each variable represents, and the spatial and temporal properties of the data. Said more plainly, the conventions explain the extra information (metadata) that clearly describes what each piece of data means and when and where it was collected.\n\n\nWhy? \nThis enables users of data from different sources to decide which quantities are comparable, and facilitates building applications with powerful extraction, regridding, and display capabilities. The CF convention includes a standard name table, which defines strings that identify physical quantities. CF is well-established, although not perfect for biology. Still, biological standards, and other standards, should consider terms from CF that can be used, before reinventing the wheel.\n\n\nKey Information (How / What)\n\nVersion 1.0 was released in October 2003, we are now on version 1.12\nCF is a convention built on top of the netCDF standard, and it generalizes and extends the netCDF COARDS conventions.\nLike DwC requires knowledge of the EML standard, CF requires knowledge of others standards. Because CF is a netCDF convention, it assumes the netCDF standard is being followed. And it relies on the UDUNITS system of specifying units (see Units in CF (UDUNITS) below).\nA CF principle is to be self-contained. So for example the CF Standard Names attempt to be as general and well-defined as possible, so the reader does not have to access outside sources to understand the terms.\nTo find standard names that describe your data, open up the latest Standard Name table (as HTML or XML) and search through it for words typically used for your data\nThe NERC Vocabulary Server hosts CF, and maintains mappings from CF to other vocabularies. They decompose them into more useful SKOS level semantics. For example: https://vocab.nerc.ac.uk/search_nvs/map/?vocab=P07\n\n\n\nTop 5 References\n\nhttps://cfconventions.org/\nCF GitHub Discussions: announcements, forum for community discussion, questions and answers\nCurrent proposals for changing CF (CF GitHub issues): vocabulary (including standard names), conventions, this website (including governance)\nCF GitHub organisation\nCF FAQ\nList of software for working with CF\nList of Projects and Activities that Use the CF Metadata Conventions\nPaper describing the CF data model and reference software\nOverview of CF basics as a presentation and paper\nhttps://www.ogc.org/standard/netcdf/;  \nhttps://gdal.org/en/latest/drivers/vector/netcdf.html\n\n\n\nRepositories that use this data standard\n\nNational Centers for Environmental Information (NCEI)\nNASA EarthData",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrate Your Data with Other Data</span>"
    ]
  },
  {
    "objectID": "INTEGRATE_DATA.html#darwin-core",
    "href": "INTEGRATE_DATA.html#darwin-core",
    "title": "3  Integrate Your Data with Other Data",
    "section": "3.2 Darwin Core",
    "text": "3.2 Darwin Core\n\nWhat Is It: \nDublin Core is a set of metadata terms used by libraries to describe physical and digital resources. \nDarwin Core (DwC) is a glossary of terms…\nThe Darwin Core archive is a set of interlinked tables…\nDarwin Core is a data standard that offers a stable, straightforward and flexible framework for compiling biodiversity data from varied and variable sources. Darwin Core is an extension of Dublin Core for biodiversity informatics. The standard was originally developed by the Biodiversity Information Standards (TDWG, formerly known as the Taxonomic Database Working Group) community, and it is currently maintained by the Darwin Core Maintenance Interest Group; feedback and participation in development is open to the public. It includes a glossary of terms intended to facilitate the sharing of information about biological diversity by providing identifiers, labels and definitions, and improving data reuse in a variety of contexts. In short, it maps information from multiple sources/institutions in a cohesive way for the broader community. Darwin Core is primarily based on taxa, their occurrence in nature as documented by observations, specimens, samples, and related information. Taxonomic occurrence data can be standardized to Darwin Core irrespective of the observing method by which the data were collected (e.g., observational data, genomics, imaging, animal tracking). Through the use of common terminology and controlled vocabularies, downstream users can more easily discover, search, evaluate, integrate and compare datasets.\n\n\nWhy?\nBiodiversity data, be it in museum collections, environmental monitoring programs, research programs or civic science projects (e.g. iNaturalist), is collected and managed in many different systems and environments. Additionally, the data is often very heterogeneous within and across these systems, depending on research objectives. Formatting data to the Darwin Core standard plays a fundamental role in facilitating open-access biodiversity data sharing, use and reuse. \nPeople collect data in all sorts of ways, but there are consistent aspects of those: the what, where, and when, and then details of those. Aligning your data allows users to put all of these disparate collection methods together. The data you collect has details with names you’ve given them, but there are equivalents with specific names that you can give them to align them.\n\n\nKey Information (How / What)\nIn practice, adopting the Darwin Core standard revolves around a standard file format, the Darwin Core Archive (DwC-A). This is the biodiversity informatics data standard that uses the Darwin Core terms to produce a data record for biodiversity data. Essentially, DwC-A is a compressed (.zip) file that contains interconnected text (e.g. csv or tsv) files that enables data publishers to share data using common terminology. These data files are logically arranged in a star-like manner, and typically consist of a core file with one or more extension files, connected through the use of primary and foreign keys. \nAside from text data tables, a DwC-A contains .xml files that facilitate human- and machine-interpretation of the data. A descriptor file (meta.xml) describes the contents of the compressed file, as well as the relationships between the core and any extensions. An eml.xml file describes the datasets contained in the DwC-A.\n\n\nTop 5 References\n\nDarwin Core Quick Reference\nWieczorek et al., (2012) - Darwin Core: An Evolving Community-Developed Biodiversity Data Standard: https://doi.org/10.1371/journal.pone.0029715 \nEcological Metadata Language\n\n\n\nRepositories that use this data standard\n\nGlobal Biodiversity Information Facility (GBIF)\nOcean Biodiversity Information System (OBIS)\nThe Atlas of Living Australia (ALA)\n\nAnd many more, see this list of Key projects using Darwin Core",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Integrate Your Data with Other Data</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html",
    "href": "INTEROPERABLE_DATA.html",
    "title": "4  Make Your Data Interoperable",
    "section": "",
    "text": "4.1 Catalogue of Life (COL)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#catalogue-of-life-col",
    "href": "INTEROPERABLE_DATA.html#catalogue-of-life-col",
    "title": "4  Make Your Data Interoperable",
    "section": "",
    "text": "What Is It?\nCatalogue of Life brings together information from taxonomists studying every group of organisms to construct an integrated view of currently accepted species across all taxonomic groups. A list of source datasets can be found here. The primary mission of COL is to deliver a freely accessible list of all species and their scientific name, but the tools and services offered by COL also enable taxonomists and other stakeholders to publish and revise species lists for any purpose.\n\n\nWhy should you use it?\nCommunicate across downstream users which organisms belong to the same group. Taxonomists continue to publish new (and revised) scientific names, which are a fundamental tool to help users to refer to these units of biodiversity, and understand everything that has been learned about its biology, distribution and relevance to mankind. CoL adds persistent identifiers that will enable users to track changes to a scientific name. \n\n\nHow to use it?\nUsers can browse the COL Checklist, which is updated monthly. COL pulls information from specific data sources, e.g. FishBase (see: https://www.catalogueoflife.org/data/taxon/49JFH). COL also has a data pipeline outlining how to best use and manage the taxonomic checklist data held by COL: https://www.catalogueoflife.org/about/colpipeline. COL also has a ChecklistBank API: https://api.checklistbank.org/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#integrated-taxonomic-information-system-itis",
    "href": "INTEROPERABLE_DATA.html#integrated-taxonomic-information-system-itis",
    "title": "4  Make Your Data Interoperable",
    "section": "4.2 Integrated Taxonomic Information System (ITIS)",
    "text": "4.2 Integrated Taxonomic Information System (ITIS)\n\nWhat Is It?\nITIS is a partnership of federal agencies that provides reliable information on taxonomy of plants, animals, fungi and microbes in North America and the world. ITIS has information on over 1.8 million species!\n\n\nWhy should you use it?\nITIS couples each scientific name with a unique taxonomic serial number (TSN) which ensures consistency and accuracy in the naming and classification of species. ITIS includes information on nomenclature, taxonomy, and distribution of species. This is an important tool for identifying and cataloging species and monitoring their populations. \n\n\nHow to use it?\nUsers can browse on the ITIS website and through the API.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#paleobiology-database-pbdb",
    "href": "INTEROPERABLE_DATA.html#paleobiology-database-pbdb",
    "title": "4  Make Your Data Interoperable",
    "section": "4.3 Paleobiology Database (PBDB)",
    "text": "4.3 Paleobiology Database (PBDB)\n\nWhat Is It?\nThe Paleobiology Database (PBDB) is an online, expert-curated database that aims to provide taxonomic information for paleobiological taxa of all geological ages. It contains data for almost half a million paleobiological taxa from over 900 different contributors.\n\n\nWhy should you use it?\nChecking your paleobiological taxonomic names against the PBDB will ensure the names are up-to-date based on current taxonomic literature. PBDB also provides the taxonomic backbone to the Global Biodiversity Information Facility (GBIF) so aligning your taxonomic names with PBDB will make the process of sharing your data easier.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#how-to-use-it-2",
    "href": "INTEROPERABLE_DATA.html#how-to-use-it-2",
    "title": "4  Make Your Data Interoperable",
    "section": "4.4 How to use it?",
    "text": "4.4 How to use it?\nPBDB can be accessed via their website, a mobile application, and an API. The PBDB website has a Resources tab where more information about these access points can be found. The same Resources page also includes information on how to contribute taxonomic information to PBDB.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "INTEROPERABLE_DATA.html#world-register-of-marine-species-worms",
    "href": "INTEROPERABLE_DATA.html#world-register-of-marine-species-worms",
    "title": "4  Make Your Data Interoperable",
    "section": "4.5 World Register of Marine Species (WoRMS)",
    "text": "4.5 World Register of Marine Species (WoRMS)\n\nWhat Is It?\nThe World Register of Marine Species (WoRMS) is an authoritative and comprehensive list of names of marine organisms. In plain language…\n\n\nWhy should you use it?\nOne of the reasons WoRMS is highly thought of in the marine community is that the content of WoRMS is curated by taxonomic and thematic experts, not by database managers. Each taxonomic group is represented by an expert who has the authority over the content, and is responsible for controlling the quality of the information. Each of these main taxonomic editors can invite several specialists of smaller groups within their area of responsibility to join them. WoRMS is the taxonomic database used by OBIS, and other important biological initiatives.\n\n\nHow to use it?\nWoRMS, and its associated tools, can be explored through your web browser, and through its API using one of the R packages (e.g. worrms, taxize) or python package.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Make Your Data Interoperable</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html",
    "title": "5  Make Your Data Internet Ready",
    "section": "",
    "text": "5.1 Web Services",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#web-services",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#web-services",
    "title": "5  Make Your Data Internet Ready",
    "section": "",
    "text": "What Is It?\nWeb services run much of our digital world today. For example, you may be familiar with Amazon Web Services (AWS), which is used for analytics and data service during football games, the olympics, and other sporting events. You can think of a web service as a waiter at a restaurant. You (the user) order food (a request), the waiter (the web service) takes your order to the kitchen (the server or application), and then brings you back your food (the response). This allows different parts of a computer system or different systems altogether to interact without needing to know how each other works internally.\n\n\nWhy should you know about them?\nBiological services and platforms like OBIS, GBIF, etc. utilize standard web services to serve data.\nThese web services are relevant to all sectors collecting and handling biological, biodiversity, and environmental information, including the private, academic research, government and other operational entities. This is what drives the exchange of scientific information.\n\n\nWeb Services to Know About\n\nApplication Program Interfaces (APIs)\nOverview: Distributed Model Data Access",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#erddap",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#erddap",
    "title": "5  Make Your Data Internet Ready",
    "section": "5.2 ERDDAP™",
    "text": "5.2 ERDDAP™\n\nWhat Is It?\nERDDAP™ [ur-dap] is a data server that offers users a simple, consistent way to download scientific datasets in common file formats, as well as make graphs and maps. ERDDAP is used globally to share and integrate disparate data across a range of communities in a standardized way. ERDDAP is often the data service used for oceanographic and atmospheric datasets, but also works great for biological and biodiversity-relevant observations, and for both gridded and tabular data. Data providers can set up their own ERDDAP server to serve up their data. By using ERDDAP you can incorporate multiple data subsets from different sources into a single workspace. Users can download data from ERDDAP in a multitude of file formats, or as graphs or maps by either using a web page or using the RESTful API in the programming language of their choice.\nTo facilitate comparisons of data from different datasets, requests and results in ERDDAP use standardized space/time axis, which makes it easier for users to specify data constraints in requests without having to worry about the data format. \nData access: ERDDAP provides a variety of data access methods including via a web browser, OPeNDAP, SOS, WMS, WCS, HTTP, and more. \nData formats: ERDDAP can convert data to various formats such as .csv, .json, .nc, .xls, .mat, .dods, and others (more info here)\nData subsetting: ERDDAP allows users to request a subset of a dataset. It converts the subset to the desired file format available for download.\nERDDAP API: All the information, data and figures made available via ERDDAP are also available via an API. See table dataset API docs here, and for gridded datasets here.\nData search: Search for data across multiple ERDDAP installations at https://erddap.com\nAdditional overall documentation on ERDDAP can be found here. The documentation is in the process of being moved to GitHub here.\n\n\nWhy should you use it?\nERDDAP is free and open source, and makes your data much more accessible. ERDDAP has a RESTful web service which is designed to be easy for computer programs and scripts to use or interact with. \n\n\nHow to use it?\nThere are many good tutorials and references on how to use ERDDAP including\n\nCoastWatch Training and specifically ERDDAP basics\nAwesome ERDDAP",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#thematic-real-time-environmental-distributed-data-services-thredds",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#thematic-real-time-environmental-distributed-data-services-thredds",
    "title": "5  Make Your Data Internet Ready",
    "section": "5.3 Thematic Real-time Environmental Distributed Data Services (THREDDS)",
    "text": "5.3 Thematic Real-time Environmental Distributed Data Services (THREDDS)\n\nWhat Is It?\nThe THREDDS server has features and interfaces that makes it easier to explore and use data. Here is a comparison of ERDDAP and THREDDS: https://jsimkins2.github.io/geog473-673/thredds-and-erddap.html \n [you can’t access data by date, you need to know the index number. Less like a database than ERDDAP. Developed a little earlier than ERDDAP. ERDDAP was built to be better than THREDDS from a user perspective. Optimized for data cube, e.g. netCDF, data.  \n\n\nWhy should you use it?\n\n\nHow to use it?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#web-map-service",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#web-map-service",
    "title": "5  Make Your Data Internet Ready",
    "section": "5.4 Web Map Service",
    "text": "5.4 Web Map Service\n\nWhat Is It?\nA Web Map Service (WMS) is a way to retrieve georegistered map images over the internet to display in applications and web pages. It allows you to view and use maps from different sources that host the maps and data used to create them without needing to download them. The WMS specifications were developed by the Open Geospatial Consortium (OGC) to enable interoperability and use in web browsers, open-source GIS software (ex. QGIS), and proprietary GIS software (ex. Esri).\n\n\nWhen should you use it?\n\n\nHow to use it?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#web-friendly-standards",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#web-friendly-standards",
    "title": "5  Make Your Data Internet Ready",
    "section": "5.5 Web-friendly Standards",
    "text": "5.5 Web-friendly Standards\n\nWhat Is It?\nWeb-friendly standards are data standards which facilitate the transfer and handling of data over the Web, its architectures and its services. Data standards that comply with web standards promote online sharing, programmatic discovery, access, and processing of data. \n\n\nWhy should you use them?\nAdopting web-friendly standards such as W3C standards, Dublin Core, the DataCite metadata schema, and https://schema.org/ helps leverage web technologies to connect and make discoverable research information across various platforms and disciplines, advancing knowledge. \nIf data standards aren’t web-friendly, data and information will be much harder to “see” via Web services, which are the primary route for global data discovery.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#web-enabled-standards-to-know-about",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#web-enabled-standards-to-know-about",
    "title": "5  Make Your Data Internet Ready",
    "section": "5.6 Web-Enabled Standards to Know About",
    "text": "5.6 Web-Enabled Standards to Know About\nThe following standards use Web-friendly and FAIR compliant approaches to promote conformance and interoperability. For example, the use of open and dereferenceable URIs/URLs as persistent identifiers for their properties and types / classes. This means that any web browser or service will be able to act on these standards and link users to the issuing authorities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#w3c-standards",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#w3c-standards",
    "title": "5  Make Your Data Internet Ready",
    "section": "5.7 W3C standards",
    "text": "5.7 W3C standards\n\nWhat Is It?**\nW3C standards) define an open web platform for application development. The web has the unprecedented potential to enable developers to build rich interactive experiences, that can be available on any device.\nThe platform continues to expand, but web users have long ago rallied around HTML as the cornerstone of the web. Many more technologies that W3C and its partners are creating extend the web and give it full strength, including CSS, SVG, WOFF, WebRTC, XML, and a growing variety of APIs.\n\n\nWhy should you use it?\nIt might not be a question so much of why you should use it as much as that you should become aware that you already use these, and may be able to make your data more FAIR by increasing your awareness of these standards. As the W3C website says, “W3C’s proven web standards process is based on fairness, openness, royalty-free, we make the web work, for everyone”.\n\n\nHow to use it?\nYou may use on of the W3C standards listed above to do activities like rendering web pages, web architecture, and linking data and services. There are a variety of resources available that can be found by searching the internet. We do not currently have a single starting point to recommend here.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#dublin-core",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#dublin-core",
    "title": "5  Make Your Data Internet Ready",
    "section": "5.8 Dublin Core",
    "text": "5.8 Dublin Core\n\nWhat Is It?\nDublin Core is a metadata standard of 15 ‘core’ terms originally developed for archives and libraries to describe physical or digital resources. The full set of terms can be found here. Each term is optional can be used multiple times, as repeated ‘elements.’ All terms are defined as Resource Description Framework (RDF) properties. It has also been formally standardized internationally as ISO 15863.\nThe Darwin Core is based on Dublin Core and is considered to be an extension for biodiversity information of Dublin Core. For information on further extensions to Darwin Core to capture details of additional information regarding a biological/biodiversity data record, use:\n\nExtended Measurement Or Facts (eMoF): (https://rs.obis.org/obis/terms#ExtendedMeasurementOrFact)\n\neMoF was developed to be used in combination with the Event Core, but is also compatible with other cores. The eMoF can store measurements or facts related to a biological occurrence, environmental measurements or facts and sampling method attributes. This extension also provides the option to provide identifiers to reference a vocabulary for the measurementType, measurementValue and measurementUnit fields.\n\nThe Humboldt Extension for Ecological Inventories (https://eco.tdwg.org/): a standard vocabulary maintained by the Darwin Core Maintenance Group. It is intended to facilitate recording of biodiversity ancillary information in operational settings.\n\n\n\nWhy should you use it?\n\n\nHow to use it?\nDisambiguating the Cores presentation: https://docs.google.com/presentation/d/1DveHXvY5U5XISl0JocDJ5qUe4PP74dPFSrdryra2m1U/edit#slide=id.p",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "MAKE_YOUR_DATA_INTERNET_READY.html#datacite",
    "href": "MAKE_YOUR_DATA_INTERNET_READY.html#datacite",
    "title": "5  Make Your Data Internet Ready",
    "section": "5.9 DataCite",
    "text": "5.9 DataCite\n\nWhat Is It?\nThe DataCite metadata schema is an international not-for-profit organization which aims to improve data citation, through helping people use web-enabled standards to connect products and citations. This allows users to i) establish easier access to research data, ii) support data archiving and long-term data preservation, and iii) increase acceptance of research data as legitimate, citable contributions to a scholarly record, promoting reuse and attribution. DataCite helps mint persistent identifiers, such as digital object identifiers (DOI) to research products, as well as provide recommendations for data citation formats. DOIs are a type of persistent identifier that identify and locate objects in the long-term. \nAdditional documentation on DataCite can be found at https://support.datacite.org/docs/datacite-commons. \n\n\nWhy should you use it?\nAssigning a DOI to your research product can enable long-term preservation and accessibility of your research product. Among other things, a main value of DataCite is the connection it creates between users and publishing machinery.\n\n\nHow to use it?\nDataCite Consortium members can mint DOIs for their research products, making it part of a larger digital ecosystem and helping connect the research product to researchers through other persistent identifiers e.g., ORCIDs for researchers and ROR for organizations. DataCite has a REST API that enables retrieval, creation, and update of a DataCite DOI metadata record, can be queried: https://support.datacite.org/reference/introduction \n##Schema.org\n\n\nWhat Is It?\nSchema.org is a set of extensible schemas that enables users to embed structured data on their web pages for use and indexation by major search engines. Markup on the webpages or in records helps search engines understand the information presented within and provide richer search results. Schema.org was launched in 2011 by Bing, Google and Yahoo to create and support a common set of schemas for structured data markup on webpages. By using schema.org vocabulary as well as various formats (e.g., JSON-LD) to mark up website content with metadata about itself, making it easier for websites or data records to be searched or indexed. \nSchema.org is not a formal standards body, but rather a site where there is documentation on the schemas supported by several major search engines. Schema.org essentially defines a dictionary of terms (types, properties, and enumerated values). Its main hierarchy is formed by a collection of types (or class), each of which has properties that describe the type. Schema.org offers a hierarchically structured set of types which determines which properties can be assigned to a particular type. Essentially this helps search engines look for websites and record and understand the relationships between them. \nDocumentation on schema.org can be found here: https://schema.org/docs/documents.html \n\n\nWhy should you use it?\nBy implementing schema.org you can make your research more easily and prominently discoverable through major search engines. \n\n\nHow to use it?\nYou can add schema.org markup to your webpages or records using various online tools, including Google’s Structured Data Markup Helper, or by directly adding code to your webpages. You can use different formats to add information to your web content implementing the schema.org vocabulary, such as JSON-LD.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Make Your Data Internet Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html",
    "href": "SOFTWARE_READY.html",
    "title": "6  Make Your Data Software Ready",
    "section": "",
    "text": "6.1 Use non-proprietary formats",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#use-non-proprietary-formats",
    "href": "SOFTWARE_READY.html#use-non-proprietary-formats",
    "title": "6  Make Your Data Software Ready",
    "section": "",
    "text": "Why?\n\nAllows data to be useful in perpetuity by ensuring data readability and reusability across multiple platforms.\nTo align better with the FAIR data principles (findability, accessibility, interoperability, reusability)\nMakes data more socially equitable, supporting open science. Proprietary formats can depend on software that require licenses, which not everyone can afford/has access to.\n\n\n\nKey Information\n\nNon-proprietary formats are supported by more than one developer and can be accessed with different software systems. For example, comma separated values (CSV) format is becoming an increasingly popular non-proprietary format.\nA proprietary file format is a file format of a company, organization, or individual that contains data that is ordered and stored according to a particular encoding-scheme, designed by the company or organization to be secret or with restricted access, such that the decoding and interpretation of this stored data is easily accomplished only with particular software or hardware that the company itself has developed. There may also be costs associated with it and access may be limited. Examples include Microsoft Excel (xlsx) and ESRI shapefiles (shp).\nMany applications (e.g. Microsoft Office) allow exporting in multiple formats.\n\n\n\nTop References\n\nTable of commonly used formats for common data types\nhttps://guides.osu.edu/c.php?g=707751&p=5027409\nA more detailed table that is specific to US Federal records management\nhttps://www.archives.gov/records-mgmt/policy/transfer-guidance-tables.html",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#structure-tabular-data-in-tidylong-format",
    "href": "SOFTWARE_READY.html#structure-tabular-data-in-tidylong-format",
    "title": "6  Make Your Data Software Ready",
    "section": "6.2 Structure tabular data in tidy/long format",
    "text": "6.2 Structure tabular data in tidy/long format\n\nWhy?\nThis is specifically intended for tabular data\n\nThere is a clear and easy to understand structure that can make your data more machine readable and easier to analyze/visualize\n\nClear structure: one observation per row\nData are as atomic as possible (e.g., don’t mix types in field)\n\nIn the biological data community, tidy formats are more likely to work with commonly-used software\nEasier to aggregate data across multiple files\n\n\n\nKey Information\nExample of Wide Format\n\n\nspeciessite_01site_02site_03Tilia americana214Pinus strobus141\n\n\nExample of Long Format\n\n\nspeciessitecountTilia americanasite_014Tilia americanasite_023Tilia americanasite_033Pinus strobussite_012Pinus strobussite_024Pinus strobussite_034\n\n\n\nCan be tricky working with multiple column datatypes\nDon’t use colors or text formatting in tabular data, and only include column names as metadata. All other notes, definitions, etc. should be in an external metadata file (e.g. data dictionary)\n\n\n\nTop References\n\nWickham, H. (2014). Tidy Data. Journal of Statistical Software, 59(10), 1–23.\nhttps://doi.org/10.18637/jss.v059.i10\nData Sharing and Management Snafu in 3 Short Acts (video)\nhttps://www.youtube.com/watch?v=N2zK3s=Atr-4&t=7s\nTips for working with data in BASH\nhttps://www.datafix.com.au/BASHing/2022-01-12.html\nData Organization in Spreadsheets for Ecologists\nhttps://datacarpentry.org/spreadsheet-ecology-lesson/\nCleaning Data and Quality Control\nhttps://edirepository.org/resources/cleaning-data-and-quality-control#data-table-structure",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#follow-iso-8601-for-dates",
    "href": "SOFTWARE_READY.html#follow-iso-8601-for-dates",
    "title": "6  Make Your Data Software Ready",
    "section": "6.3 Follow ISO 8601 for dates",
    "text": "6.3 Follow ISO 8601 for dates\n\n\n\n\n\nhttps://imgs.xkcd.com/comics/iso_8601.png\n\n\n\n\n\nWhy?\n\nInternationally accepted format used across multiple schemas (e.g. Darwin Core, EML, ISO 19115)\nRemoves ambiguity related to timezone, daylight savings time changes, and time of day\nBetter software integration of time date/time elements\n\n\n\nKey Information\n\nUTC (AKA Zulu or GMT): Coordinated Universal Time (UTC) is the primary time standard by which the world regulates clocks and time. It is time relative to 0° longitude and is not adjusted for daylight saving time. (from Wikipedia).\nConversion to UTC, or between time zones, may depend on daylight savings\n\nExamples: April 3, 2023 standardized to ISO 8601\n\n\nDescriptionWritten in ISO 8601Date2023-04-03Date and Time with timezone offset2023-04-03T18:29:38+00:00Date and Time in UTC2023-04-03T18:29:38ZTime Interval in UTC (April 3 - 5, 2023)2023-04-03T18:29:38Z/2023-04-05T00:29:38Z\n\n\nExamples: different styles of timezone annotation\n\n\nDescriptionWritten in ISO 8601Date2023-04-03Date and Time with timezone offset2023-04-03T18:29:38+00:00Date and Time in UTC2023-04-03T18:29:38ZTime Interval in UTC (April 3 - 5, 2023)2023-04-03T18:29:38Z/2023-04-05T00:29:38Z\n\n\n\n\nTop References\n\nISO 8601 wiki: https://en.wikipedia.org/wiki/ISO_8601\nR package lubridate, OlsonNames()\nPython go-to package, datetime https://docs.python.org/3/library/datetime.html\nArticle on datetime uncertainty: https://www.datafix.com.au/BASHing/2020-02-12.html\nMap of offset from UTC: https://www.timeanddate.com/time/map/\nNice time converter: https://coastwatch.pfeg.noaa.gov/erddap/convert/time.html",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#match-scientific-names-to-a-taxonomic-authority",
    "href": "SOFTWARE_READY.html#match-scientific-names-to-a-taxonomic-authority",
    "title": "6  Make Your Data Software Ready",
    "section": "6.4 Match scientific names to a taxonomic authority",
    "text": "6.4 Match scientific names to a taxonomic authority\n\nWhy?\n\nTo integrate or aggregate datasets, we need a common frame of reference for taxonomic name\nProvides an anchor for the taxonomy as scientific understanding evolves.\n\n\n\nKey Information\n\nDefinition: As used here, a taxonomic authority is an online resource that maintains up-to-date species-level classification information and provides persistent identifiers for taxonomic classifications. Example: For the species Balaenoptera borealis (Lesson, 1828), the WoRMS taxonomic authority ID link is https://www.marinespecies.org/aphia.php?p=taxdetails&id=137088 and the LSID is urn:lsid:marinespecies.org:taxname:137088.\nUse an existing taxonomic authority (e.g. World Register of Marine Species , Integrated Taxonomic Information System , NCBI taxonomy) and include the authority who manages said information in your metadata\nList of many authorities can be found here: https://resolver.globalnames.org/data_sources\nMake yourself aware of the structure, limits, and history of the authority you are using.\nAdopt standard binomial nomenclature, when possible\nWhen possible, reference the unique identifier in addition to the nomenclature.\nAlways save and document the originally recorded name.\nPut notes about identification uncertainty in a separate column.\nMany authorities have APIs through which you can match names to identifiers.\n\n\n\nTop References\n\nR packages\n\ntaxize is a taxonomic toolbelt for R. taxize wraps APIs for a large suite of taxonomic databases available on the web\nhttps://cran.r-project.org/web/packages/taxize/index.html\nworrms is an API client for World Register of Marine Species\nhttp://cran.nexr.com/web/packages/worrms/vignettes/worrms_vignette.html\nworms: another API client for WoRMS\nhttps://cran.r-project.org/web/packages/worms/index.html\nRitis: API client for ITIS &lt;https://cran.r-project.org/web/packages/ritis/&gt;\n\nPython packages\n\nWoRMS API client\nhttps://github.com/iobis/pyworms\n\nGlobal Names Resolver to compare taxonomic concepts across authorities\nhttps://resolver.globalnames.org/\nArticle: Recommendations for the Standardisation of Open Taxonomic Nomenclature for Image-Based Identifications\nhttps://doi.org/10.3389/fmars.2021.620702\nTDWG 2022 Keynote: Richard Pyle, “An Introduction to the Scientific Names of Organisms and the Taxon Concepts they Represent”\nhttps://www.youtube.com/watch?v=rmTvUUjBxrI",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#record-latitude-and-longitude-in-decimal-degrees-in-wgs84",
    "href": "SOFTWARE_READY.html#record-latitude-and-longitude-in-decimal-degrees-in-wgs84",
    "title": "6  Make Your Data Software Ready",
    "section": "6.5 Record latitude and longitude in decimal degrees in WGS84",
    "text": "6.5 Record latitude and longitude in decimal degrees in WGS84\n\n\n\n\n\nhttps://imgs.xkcd.com/comics/coordinate_precision.png\n\n\n\n\n\nWhy?\n\nUsers have to know where you collected this data, which requires a latitude, longitude, reference system and uncertainty.\nDecimal-degrees avoids special symbols (° or ‘) which is preferable for machine readable formats\nWGS84 is a reference coordinate system that is widely used and incorporated in many GPS units and tools, and recognized as a standard by many government agencies.\n\n\n\nKey Information\n\nIf possible, encourage data providers to confirm, and record, the WGS84 datum prior to data collection.\nUnderstand and report the device/instrument uncertainty associated with your coordinates because it affects the usability of your data.\nConsider including the vertical component (altitude, depth, height off bottom, elevation, etc)\nGenerally speaking, degrees-minutes-seconds (DMS) can be converted to decimal-degrees (DD) by:\n\nDD = d + (min/60) + (sec/3600)\nWatch out for mixed formats, like degrees, decimal-minutes (DDM).\n\nDegrees West and South become negative in DD.\n\nValues for longitude range from -180 to 180, inclusive.\nValues for latitude range from -90 to 90, inclusive.\n\n\nExample Coordinates\n\n\nFormatExampleDecimal Degrees (DD)30.50833333Degrees Minutes Seconds (DMS)30° 15' 10 NDegrees Decimal Minutes (DM or DDM)30° 15.1667 N\n\n\n\n\nTop References\n\nExisting R/python/ESRI packages/functions\n\nR - measurements https://cran.r-project.org/web/packages/measurements/measurements.pdf\nEML https://eml.ecoinformatics.org/schema/index.html (find “bounding Coordinates)\nCF https://cfconventions.org/Data/cf-conventions/cf-conventions-1.10/cf-conventions.html#latitude-coordinate\n\nGetting lat/lon to decimal degrees\nhttps://ioos.github.io/bio_mobilization_workshop/03-data-cleaning/index.html#getting-latlon-to-decimal-degrees\nSome background on precision\n\nhttps://www.trekview.org/blog/2021/reading-decimal-gps-coordinates-like-a-computer/#a-note-on-accuracy\nhttps://gis.stackexchange.com/questions/8650/measuring-accuracy-of-latitude-and-longitude\n\nDMS to DD calculator\nhttps://www.fcc.gov/media/radio/dms-decimal – The three most commonly used datums are WGS84, NAD83, and NAD27. A more complete list can be found here: https://wiki.gis.com/wiki/index.php/Datum_(geodesy)#List_of_Datums)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  },
  {
    "objectID": "SOFTWARE_READY.html#use-persistent-unique-identifiers",
    "href": "SOFTWARE_READY.html#use-persistent-unique-identifiers",
    "title": "6  Make Your Data Software Ready",
    "section": "6.6 Use persistent unique identifiers",
    "text": "6.6 Use persistent unique identifiers\n\nWhy?\n\nIt can be useful to have unique identifiers to unambiguously identify granules of information, e.g. dataset, collection, database, taxonomic concept, etc. This will allow users to precisely refer to the data and allow your data to remain identifiable when aggregated with other datasets.\nTo be able to uniquely identify a record in your data system or across data systems. Useful to create relational databases or merge records.\nAlthough it increases workload, it safeguards against confusion and inefficiency in the future.\n\n\n\nKey Information\n\nThere are good reasons to keep an identifier opaque, i.e. it does not indicate anything about the content of information it points to. However, there are also transparent, or semi-opaque identifiers in use that take advantage of semantics to guide humans as well as machines.\nOne way to create a unique identifier is concatenation of sampling event, location, time, enumeration of unique observation or event. (e.g. Station_95_Date_09JAN1997:14:35:00.000)\nSome prefer using opaque identifiers. (e.g. 10FC9784-B30F-48ED-8DB5-FF65A2A9934E)\nIf there is an existing persistent unique identifier, it’s usually a good idea to use it (i.e. when using a taxonomic authority like WoRMS and applying their LSID).\nIt is important to manage any identifiers you create, if they are not managed by an authority (e.g. DOIs).\nImportant that it be persistent (consider samples possibly moving between institutions)\n\nExamples of PIDs\n\n\nType of PIDUse CaseExampleDigital Object Identifier (DOI)Actionable persistent link for papers, data, and other digital objectshttps://doi.org/10.6084/m9.figshare.16806712.v2International Geo Sample Number (IGSN)Persistent identifier for physical sampleshttp://igsn.org/AU1243&gt;Life Science Identifier (LSID)Persistent structured method for biologically significant dataurn:lsid:marinespecies.org:taxname:218214Open Researcher and Contributor ID (ORCID)Persistent actionable link for individualshttps://orcid.org/0000-0002-4391-107X\n\n\n\n\nTop References\n\nSoftware and Packages to generate uuids:\n\nR - uuid https://cran.r-project.org/web/packages/uuid/index.html\npython - uuid https://docs.python.org/3/library/uuid.html\nhttp://guid.one/\nhttps://guidgenerator.com/\n\nGuidance on how to use GUIDs (Globally Unique Identifiers) to meet specific requirements of the biodiversity information community\nhttp://bioimages.vanderbilt.edu/pages/guid-applicability-final-2011-01.pdf\nUse of globally unique identifiers (GUIDs) to link herbarium specimen records to physical specimens\nhttps://bsapubs.onlinelibrary.wiley.com/doi/full/10.1002/aps3.1027\nA Beginner’s Guide to Persistent Identifiers\nhttp://links.gbif.org/persistent_identifiers_guide_en_v1.pdf",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Make Your Data Software Ready</span>"
    ]
  }
]